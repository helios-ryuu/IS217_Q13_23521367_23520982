"""
H·ªá th·ªëng ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cu·ªëi c√πng - T·ªëi ∆∞u h√≥a cho SQL Server
T√°c gi·∫£: Final preprocessing system
Ng√†y: 2024
"""

import pandas as pd
import numpy as np
import warnings
import os
import gc
import re
from datetime import datetime
from typing import List, Optional, Dict
from tqdm import tqdm

# T·∫Øt c·∫£nh b√°o
warnings.filterwarnings('ignore', category=FutureWarning)
pd.options.mode.chained_assignment = None

# ==========================================
# C√°c Pha Ti·ªÅn X·ª≠ L√Ω Thu·∫ßn T√∫y
# ==========================================

def phase_delete_columns(df: pd.DataFrame, columns_to_delete: List[str]) -> pd.DataFrame:
    """Pha 1: X√≥a c·ªôt kh√¥ng c·∫ßn thi·∫øt"""
    columns_to_drop = [col for col in columns_to_delete if col in df.columns]
    return df.drop(columns=columns_to_drop) if columns_to_drop else df

def phase_filter_date(df: pd.DataFrame, time_column: str = 'Start_Time', 
                     date_cutoff: str = "2018-01-01") -> pd.DataFrame:
    """Pha 2: L·ªçc d·ªØ li·ªáu theo ng√†y"""
    if time_column not in df.columns:
        return df
    
    if df[time_column].dtype != 'datetime64[ns]':
        df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
    
    cutoff_date = pd.to_datetime(date_cutoff)
    return df[df[time_column] >= cutoff_date]

def phase_create_time_features(df: pd.DataFrame, time_column: str = 'Start_Time') -> pd.DataFrame:
    """Pha 3: T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian"""
    if time_column not in df.columns:
        return df
    
    df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
    
    # T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian c∆° b·∫£n
    df['YEAR'] = df[time_column].dt.year.astype('int16')
    df['QUARTER'] = df[time_column].dt.quarter.astype('int8')
    df['MONTH'] = df[time_column].dt.month.astype('int8')
    df['DAY'] = df[time_column].dt.day.astype('int8')
    df['HOUR'] = df[time_column].dt.hour.astype('int8')
    df['MINUTE'] = df[time_column].dt.minute.astype('int8')
    df['SECOND'] = df[time_column].dt.second.astype('int8')
    df['IS_WEEKEND'] = df[time_column].dt.dayofweek.isin([5, 6]).astype('bool')
    
    return df.drop(columns=[time_column])

def phase_sql_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 4: Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu SQL Server"""
    
    # T·ªça ƒë·ªô: decimal(9,6)
    coord_cols = ['Start_Lat', 'Start_Lng', 'LATITUDE', 'LONGITUDE']
    for col in coord_cols:
        if col in df.columns:
            df[col] = df[col].round(6).astype('float64')
    
    # S·ªë th·ª±c kh√°c: decimal(8,4) 
    float_cols = df.select_dtypes(include=['float64', 'float32']).columns
    for col in float_cols:
        if col not in coord_cols:
            df[col] = df[col].round(4).astype('float64')
    
    # S·ªë nguy√™n: int/smallint/tinyint/bit
    int_cols = df.select_dtypes(include=['int64', 'int32']).columns
    for col in int_cols:
        if col in ['YEAR']:
            df[col] = df[col].astype('int16')  # smallint
        elif col in ['QUARTER', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND']:
            df[col] = df[col].astype('int8')   # tinyint
        else:
            df[col] = df[col].astype('int32')  # int
    
    # Boolean -> bit
    bool_cols = df.select_dtypes(include=['bool']).columns
    for col in bool_cols:
        df[col] = df[col].astype('int8')  # bit trong SQL
    
    # Chu·ªói: nvarchar(4000) cho STREET, nvarchar(100) cho c√°c c·ªôt kh√°c
    string_cols = df.select_dtypes(include=['object']).columns
    for col in string_cols:
        df[col] = df[col].astype('string')  # S·ª≠ d·ª•ng string type ƒë·ªÉ mapping SQL
    
    return df

def phase_standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 5: Chu·∫©n h√≥a t√™n c·ªôt"""
    column_mapping = {}
    for col in df.columns:
        new_col = col.upper()
        new_col = re.sub(r'\([^)]*\)', '', new_col)
        new_col = re.sub(r'\s+', '_', new_col.strip())
        new_col = re.sub(r'_+', '_', new_col).strip('_')
        column_mapping[col] = new_col
    
    df = df.rename(columns=column_mapping)
    
    # ƒê·ªïi t√™n t·ªça ƒë·ªô c·ª• th·ªÉ
    coordinate_mapping = {'START_LAT': 'LATITUDE', 'START_LNG': 'LONGITUDE'}
    for old_name, new_name in coordinate_mapping.items():
        if old_name in df.columns:
            df = df.rename(columns={old_name: new_name})
    
    return df



def phase_validate_clean(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 6: X√°c th·ª±c v√† l√†m s·∫°ch d·ªØ li·ªáu"""
    
    def find_column(patterns: List[str]) -> Optional[str]:
        for pattern in patterns:
            for col in df.columns:
                if pattern.upper() in col.upper():
                    return col
        return None
    
    # X√°c th·ª±c m·ª©c ƒë·ªô nghi√™m tr·ªçng
    severity_col = find_column(['SEVERITY'])
    if severity_col and severity_col in df.columns:
        df = df[df[severity_col].isin([1, 2, 3, 4])]
    
    # X√°c th·ª±c v√† lo·∫°i b·ªè b·∫£n sao
    lat_col = find_column(['LATITUDE', 'START_LAT'])
    lng_col = find_column(['LONGITUDE', 'START_LNG'])
    
    if lat_col and lng_col and lat_col in df.columns and lng_col in df.columns:
        time_cols = [find_column([col]) for col in ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE']]
        time_cols = [col for col in time_cols if col and col in df.columns]
        
        if len(time_cols) >= 4:
            # L√†m tr√≤n t·ªça ƒë·ªô ƒë·ªÉ ph√°t hi·ªán b·∫£n sao
            df_temp = df.copy()
            df_temp[f'{lat_col}_rounded'] = df_temp[lat_col].round(4)
            df_temp[f'{lng_col}_rounded'] = df_temp[lng_col].round(4)
            
            duplicate_cols = [f'{lat_col}_rounded', f'{lng_col}_rounded'] + time_cols
            duplicate_mask = df_temp.duplicated(subset=duplicate_cols, keep='first')
            df = df[~duplicate_mask]
    
    return df

# ==========================================
# H√†m X·ª≠ L√Ω Ch√≠nh
# ==========================================

def get_file_info(file_path: str) -> Dict:
    """L·∫•y th√¥ng tin chi ti·∫øt file"""
    if not os.path.exists(file_path):
        return {"size_mb": 0, "exists": False}
    
    size_bytes = os.path.getsize(file_path)
    size_mb = size_bytes / (1024 * 1024)
    size_gb = size_mb / 1024
    
    return {
        "size_bytes": size_bytes,
        "size_mb": size_mb,
        "size_gb": size_gb,
        "exists": True,
        "formatted": f"{size_mb:.1f} MB" if size_mb < 1024 else f"{size_gb:.2f} GB"
    }

def process_chunks(input_file: str, output_file: str, chunk_size: int = 2600000,
                  columns_to_delete: List[str] = None, date_cutoff: str = "2018-01-01") -> Optional[Dict]:
    """X·ª≠ l√Ω d·ªØ li·ªáu theo kh·ªëi"""
    
    if columns_to_delete is None:
        columns_to_delete = ['ID', 'Description', 'End_Lat', 'End_Lng', 'End_Time', 'Weather_Timestamp']
    
    if not os.path.exists(input_file):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {input_file}")
        return None
    
    # X√≥a file ƒë·∫ßu ra n·∫øu c√≥
    if os.path.exists(output_file):
        os.remove(output_file)
    
    # Th·ªëng k√™ x·ª≠ l√Ω
    stats = {
        'chunks_processed': 0,
        'total_rows_input': 0,
        'total_rows_output': 0,
        'columns_deleted': 0,
        'time_features_added': 7,
        'phase_stats': {}
    }
    
    first_chunk = True
    
    try:
        # ƒê·∫øm t·ªïng d√≤ng
        print("üîç ƒêang ƒë·∫øm t·ªïng s·ªë d√≤ng...")
        total_lines = sum(1 for _ in open(input_file, encoding='utf-8')) - 1
        total_chunks = (total_lines + chunk_size - 1) // chunk_size
        print(f"üìä T·ªïng {total_lines:,} d√≤ng, {total_chunks} kh·ªëi")
        
        # X·ª≠ l√Ω t·ª´ng kh·ªëi
        chunk_reader = pd.read_csv(input_file, chunksize=chunk_size, low_memory=False)
        
        with tqdm(total=total_chunks, desc="X·ª≠ l√Ω kh·ªëi", unit="kh·ªëi") as pbar:
            for chunk_num, chunk in enumerate(chunk_reader, 1):
                initial_rows = len(chunk)
                initial_cols = len(chunk.columns)
                
                # √Åp d·ª•ng c√°c pha x·ª≠ l√Ω
                try:
                    chunk = phase_delete_columns(chunk, columns_to_delete)
                    if chunk_num == 1:
                        stats['columns_deleted'] = initial_cols - len(chunk.columns)
                    
                    chunk = phase_filter_date(chunk, date_cutoff=date_cutoff)
                    if len(chunk) == 0:
                        pbar.update(1)
                        continue
                    
                    chunk = phase_create_time_features(chunk)
                    chunk = phase_sql_data_types(chunk)
                    chunk = phase_standardize_columns(chunk)
                    chunk = phase_validate_clean(chunk)
                    
                except Exception as e:
                    print(f"\n‚ùå L·ªói x·ª≠ l√Ω kh·ªëi {chunk_num}: {e}")
                    return None
                
                # L∆∞u kh·ªëi
                if first_chunk:
                    chunk.to_csv(output_file, index=False, mode='w')
                    first_chunk = False
                else:
                    chunk.to_csv(output_file, index=False, mode='a', header=False)
                
                # C·∫≠p nh·∫≠t th·ªëng k√™
                stats['chunks_processed'] = chunk_num
                stats['total_rows_input'] += initial_rows
                stats['total_rows_output'] += len(chunk)
                
                # C·∫≠p nh·∫≠t progress bar
                pbar.set_postfix({
                    'D√≤ng ƒë·∫ßu v√†o': f"{initial_rows:,}",
                    'D√≤ng ƒë·∫ßu ra': f"{len(chunk):,}",
                    'T·ªïng': f"{stats['total_rows_output']:,}"
                })
                pbar.update(1)
                
                # D·ªçn d·∫πp b·ªô nh·ªõ
                del chunk
                gc.collect()
        
        return stats
        
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω: {e}")
        return None

def analyze_dataset_detailed(file_path: str, sample_size: int = 50000) -> Optional[pd.DataFrame]:
    """Ph√¢n t√≠ch chi ti·∫øt b·ªô d·ªØ li·ªáu"""
    if not os.path.exists(file_path):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}")
        return None
    
    try:
        df_sample = pd.read_csv(file_path, nrows=sample_size, low_memory=False)
        total_rows = sum(1 for _ in open(file_path, encoding='utf-8')) - 1
        file_info = get_file_info(file_path)
        
        print(f"üìÅ File: {os.path.basename(file_path)}")
        print(f"üíæ K√≠ch th∆∞·ªõc: {file_info['formatted']}")
        print(f"üìè T·ªïng d√≤ng: {total_rows:,}")
        print(f"üìê T·ªïng c·ªôt: {len(df_sample.columns)}")
        
        # Ph√¢n t√≠ch ki·ªÉu d·ªØ li·ªáu
        print(f"\nüìä KI·ªÇU D·ªÆ LI·ªÜU:")
        dtype_counts = df_sample.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"  {dtype}: {count} c·ªôt")
        
        # Gi√° tr·ªã thi·∫øu
        missing = df_sample.isnull().sum()
        if missing.sum() > 0:
            print(f"\nüîç GI√Å TR·ªä THI·∫æU:")
            missing_pct = (missing / len(df_sample) * 100).round(2)
            for col in missing[missing > 0].head(5).index:
                print(f"  {col}: {missing[col]:,} ({missing_pct[col]}%)")
        else:
            print(f"\n‚úÖ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu!")
        
        return df_sample
        
    except Exception as e:
        print(f"‚ùå L·ªói ph√¢n t√≠ch: {e}")
        return None

def compare_datasets_detailed(original_file: str, processed_file: str, processing_stats: Dict) -> None:
    """So s√°nh chi ti·∫øt hai b·ªô d·ªØ li·ªáu"""
    print(f"\n" + "="*70)
    print("üîÑ SO S√ÅNH CHI TI·∫æT B·ªò D·ªÆ LI·ªÜU")
    print("="*70)
    
    # Th√¥ng tin file
    orig_info = get_file_info(original_file)
    proc_info = get_file_info(processed_file)
    
    print(f"üìÅ FILE:")
    print(f"  G·ªëc: {orig_info['formatted']}")
    print(f"  X·ª≠ l√Ω: {proc_info['formatted']}")
    
    if orig_info['exists'] and proc_info['exists']:
        reduction = ((orig_info['size_mb'] - proc_info['size_mb']) / orig_info['size_mb']) * 100
        print(f"  Gi·∫£m dung l∆∞·ª£ng: {reduction:.1f}%")
    
    # Th·ªëng k√™ d√≤ng
    print(f"\nüìè D·ªÆ LI·ªÜU:")
    print(f"  D√≤ng ƒë·∫ßu v√†o: {processing_stats['total_rows_input']:,}")
    print(f"  D√≤ng ƒë·∫ßu ra: {processing_stats['total_rows_output']:,}")
    row_reduction = ((processing_stats['total_rows_input'] - processing_stats['total_rows_output']) / processing_stats['total_rows_input']) * 100
    print(f"  Gi·∫£m d√≤ng: {row_reduction:.1f}%")
    print(f"  C·ªôt ƒë√£ x√≥a: {processing_stats['columns_deleted']}")
    print(f"  ƒê·∫∑c tr∆∞ng th·ªùi gian th√™m: {processing_stats['time_features_added']}")
    print(f"  Kh·ªëi ƒë√£ x·ª≠ l√Ω: {processing_stats['chunks_processed']}")

def main(input_file: str = "../US_Accidents_March23.csv",
         output_file: str = "../US_Accidents_March23-final.csv",
         chunk_size: int = 2600000,
         date_cutoff: str = "2018-01-01",
         columns_to_delete: List[str] = None) -> bool:
    """H√†m ch√≠nh"""
    
    if columns_to_delete is None:
        columns_to_delete = ['ID', 'Description', 'End_Lat', 'End_Lng', 'End_Time', 'Weather_Timestamp']
    
    print("üöÄ H·ªÜ TH·ªêNG TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU CU·ªêI C√ôNG")
    print(f"‚è∞ B·∫Øt ƒë·∫ßu: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # C·∫•u h√¨nh
    print("‚öôÔ∏è C·∫§U H√åNH:")
    print(f"  File ƒë·∫ßu v√†o: {input_file}")
    print(f"  File ƒë·∫ßu ra: {output_file}")
    print(f"  K√≠ch th∆∞·ªõc kh·ªëi: {chunk_size:,} d√≤ng")
    print(f"  Ng√†y c·∫Øt: {date_cutoff}")
    print(f"  C·ªôt x√≥a: {len(columns_to_delete)} c·ªôt")
    
    try:
        # Ph√¢n t√≠ch d·ªØ li·ªáu g·ªëc
        print(f"\nüìä PH√ÇN T√çCH D·ªÆ LI·ªÜU G·ªêC:")
        print("-" * 40)
        original_sample = analyze_dataset_detailed(input_file)
        
        # X·ª≠ l√Ω d·ªØ li·ªáu
        print(f"\nüîÑ X·ª¨ L√ù D·ªÆ LI·ªÜU:")
        processing_stats = process_chunks(
            input_file=input_file,
            output_file=output_file,
            chunk_size=chunk_size,
            columns_to_delete=columns_to_delete,
            date_cutoff=date_cutoff
        )
        
        if processing_stats is None:
            print("‚ùå X·ª≠ l√Ω th·∫•t b·∫°i")
            return False
        
        # Ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        print(f"\nüìä PH√ÇN T√çCH D·ªÆ LI·ªÜU ƒê√É X·ª¨ L√ù:")
        print("-" * 40)
        processed_sample = analyze_dataset_detailed(output_file)
        
        # So s√°nh chi ti·∫øt
        compare_datasets_detailed(input_file, output_file, processing_stats)
        
        # T·∫°o b√°o c√°o chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
        print(f"\nüìã T·∫†O B√ÅO C√ÅO CHUY·ªÇN ƒê·ªîI KI·ªÇU D·ªÆ LI·ªÜU...")
        try:
            from type_conversion import generate_type_conversion_report
            generate_type_conversion_report(output_file, processed_sample)
        except ImportError:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y type_conversion.py - b·ªè qua b√°o c√°o chuy·ªÉn ƒë·ªïi")
        
        print(f"\n" + "="*70)
        print("‚úÖ TI·ªÄN X·ª¨ L√ù HO√ÄN TH√ÄNH!")
        print(f"üìÅ K·∫øt qu·∫£: {output_file}")
        print(f"‚è∞ Ho√†n th√†nh: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)