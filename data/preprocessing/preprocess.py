import os
import re
import gc
import warnings
import argparse
import traceback
import pandas as pd
import numpy as np
from datetime import datetime
from typing import List, Optional, Dict
from tqdm import tqdm

# T·∫Øt c·∫£nh b√°o
warnings.filterwarnings('ignore', category=FutureWarning)
pd.options.mode.chained_assignment = None

# ==========================================
# Preprocessing Reporter
# ==========================================

class PreprocessingReporter:
    def __init__(self, input_file: str, output_file: str):
        """Kh·ªüi t·∫°o reporter"""
        dataset_name = os.path.splitext(os.path.basename(input_file))[0]
        self.input_file = input_file
        self.output_file = output_file
        self.report_file = os.path.join(os.path.dirname(output_file), f"{dataset_name}-preprocess_report.txt")
        self.report_content = []
        
    def add_to_report(self, content: str):
        """Th√™m n·ªôi dung v√†o b√°o c√°o"""
        self.report_content.append(content)
        
    def save_report(self) -> bool:
        """L∆∞u b√°o c√°o ra file"""
        try:
            with open(self.report_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(self.report_content))
            return True
        except Exception as e:
            return False
    
    def generate_header(self):
        """T·∫°o header cho b√°o c√°o"""
        self.add_to_report("="*80)
        self.add_to_report("üîÑ B√ÅO C√ÅO TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU")
        self.add_to_report("="*80)
        self.add_to_report(f"üìÖ Ng√†y t·∫°o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.add_to_report("")
    
    def analyze_original_dataset(self) -> Optional[Dict]:
        """Ph√¢n t√≠ch dataset g·ªëc"""
        if not os.path.exists(self.input_file):
            return None
            
        try:
            sample_df = pd.read_csv(self.input_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.input_file, encoding='utf-8')) - 1
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': get_file_info(self.input_file),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / 1024 / 1024,
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'column_names': list(sample_df.columns)
            }
            
            return analysis
            
        except Exception as e:
            return None
    
    def analyze_processed_dataset(self) -> Optional[Dict]:
        """Ph√¢n t√≠ch dataset ƒë√£ x·ª≠ l√Ω"""
        if not os.path.exists(self.output_file):
            return None
            
        try:
            sample_df = pd.read_csv(self.output_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.output_file, encoding='utf-8')) - 1
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': get_file_info(self.output_file),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / 1024 / 1024,
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'column_names': list(sample_df.columns)
            }
            
            return analysis
            
        except Exception as e:
            return None
    
    def generate_comparison_report(self, original_analysis: Dict, processed_analysis: Dict, processing_stats: Dict):
        """T·∫°o b√°o c√°o so s√°nh chi ti·∫øt"""
        
        # Th√™m th√¥ng tin c·∫•u h√¨nh
        if 'file_info' in processing_stats:
            config = processing_stats['file_info']
            self.add_to_report("‚öôÔ∏è TH√îNG TIN C·∫§U H√åNH")
            self.add_to_report("-" * 50)
            self.add_to_report(f"File ƒë·∫ßu v√†o: {config['input_file']}")
            self.add_to_report(f"File ƒë·∫ßu ra: {config['output_file']}")
            self.add_to_report(f"K√≠ch th∆∞·ªõc kh·ªëi: {config['chunk_size']:,} d√≤ng")
            self.add_to_report(f"Ng√†y c·∫Øt: {config['date_cutoff']}")
            self.add_to_report(f"C·ªôt x√≥a: {len(config['columns_to_delete'])} c·ªôt")
            self.add_to_report("")
        
        # Th√™m processing log n·∫øu c√≥
        if 'processing_log' in processing_stats and processing_stats['processing_log']:
            self.add_to_report("üìù CHI TI·∫æT QU√Å TR√åNH X·ª¨ L√ù")
            self.add_to_report("-" * 50)
            for log_entry in processing_stats['processing_log']:
                self.add_to_report(log_entry)
            self.add_to_report("")
        
        # Th√™m comparison log n·∫øu c√≥
        if 'comparison_log' in processing_stats and processing_stats['comparison_log']:
            for log_entry in processing_stats['comparison_log']:
                self.add_to_report(log_entry)
            self.add_to_report("")
        
        # Th√™m analysis g·ªëc n·∫øu c√≥
        if 'original_analysis' in processing_stats and processing_stats['original_analysis']:
            orig = processing_stats['original_analysis']
            self.add_to_report("üìä PH√ÇN T√çCH D·ªÆ LI·ªÜU G·ªêC")
            self.add_to_report("-" * 50)
            for summary in orig['analysis_summary']:
                self.add_to_report(summary)
            
            if orig['dtype_counts']:
                self.add_to_report("\nKi·ªÉu d·ªØ li·ªáu:")
                for dtype, count in orig['dtype_counts'].items():
                    self.add_to_report(f"  {dtype}: {count} c·ªôt")
            
            if orig['missing_info']:
                self.add_to_report("\nGi√° tr·ªã thi·∫øu:")
                for missing in orig['missing_info']:
                    self.add_to_report(f"  {missing}")
            else:
                self.add_to_report("\n‚úÖ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu!")
            self.add_to_report("")
        
        # Th√™m analysis ƒë√£ x·ª≠ l√Ω n·∫øu c√≥
        if 'processed_analysis' in processing_stats and processing_stats['processed_analysis']:
            proc = processing_stats['processed_analysis']
            self.add_to_report("üìä PH√ÇN T√çCH D·ªÆ LI·ªÜU ƒê√É X·ª¨ L√ù")
            self.add_to_report("-" * 50)
            for summary in proc['analysis_summary']:
                self.add_to_report(summary)
            
            if proc['dtype_counts']:
                self.add_to_report("\nKi·ªÉu d·ªØ li·ªáu:")
                for dtype, count in proc['dtype_counts'].items():
                    self.add_to_report(f"  {dtype}: {count} c·ªôt")
            
            if proc['missing_info']:
                self.add_to_report("\nGi√° tr·ªã thi·∫øu:")
                for missing in proc['missing_info']:
                    self.add_to_report(f"  {missing}")
            else:
                self.add_to_report("\n‚úÖ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu!")
            self.add_to_report("")
        
        self.add_to_report("üîç TH√îNG TIN T·ªîNG QUAN")
        self.add_to_report("-" * 50)
        
        # Th√¥ng tin c∆° b·∫£n
        self.add_to_report(f"Dataset g·ªëc:")
        self.add_to_report(f"  - S·ªë d√≤ng: {original_analysis['total_rows']:,}")
        self.add_to_report(f"  - S·ªë c·ªôt: {original_analysis['total_columns']}")
        self.add_to_report(f"  - K√≠ch th∆∞·ªõc file: {original_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - B·ªô nh·ªõ: {original_analysis['memory_usage']:.1f} MB")
        
        self.add_to_report(f"\nDataset ƒë√£ x·ª≠ l√Ω:")
        self.add_to_report(f"  - S·ªë d√≤ng: {processed_analysis['total_rows']:,}")
        self.add_to_report(f"  - S·ªë c·ªôt: {processed_analysis['total_columns']}")
        self.add_to_report(f"  - K√≠ch th∆∞·ªõc file: {processed_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - B·ªô nh·ªõ: {processed_analysis['memory_usage']:.1f} MB")
        
        # Th·ªëng k√™ thay ƒë·ªïi
        self.add_to_report(f"\nüìä THAY ƒê·ªîI SAU TI·ªÄN X·ª¨ L√ù")
        self.add_to_report("-" * 50)
        
        # Thay ƒë·ªïi s·ªë d√≤ng
        row_change = processed_analysis['total_rows'] - original_analysis['total_rows']
        row_change_pct = (row_change / original_analysis['total_rows']) * 100
        self.add_to_report(f"S·ªë d√≤ng: {row_change:+,} ({row_change_pct:+.1f}%)")
        
        # Thay ƒë·ªïi s·ªë c·ªôt
        col_change = processed_analysis['total_columns'] - original_analysis['total_columns']
        self.add_to_report(f"S·ªë c·ªôt: {col_change:+} c·ªôt")
        
        # Thay ƒë·ªïi k√≠ch th∆∞·ªõc file
        size_change_mb = processed_analysis['file_size']['size_mb'] - original_analysis['file_size']['size_mb']
        size_change_pct = (size_change_mb / original_analysis['file_size']['size_mb']) * 100
        self.add_to_report(f"K√≠ch th∆∞·ªõc file: {size_change_mb:+.1f} MB ({size_change_pct:+.1f}%)")
        
        # Thay ƒë·ªïi b·ªô nh·ªõ
        memory_change = processed_analysis['memory_usage'] - original_analysis['memory_usage']
        memory_change_pct = (memory_change / original_analysis['memory_usage']) * 100
        self.add_to_report(f"B·ªô nh·ªõ: {memory_change:+.1f} MB ({memory_change_pct:+.1f}%)")
        
        # Chi ti·∫øt c√°c pha x·ª≠ l√Ω
        self.add_to_report(f"\nüîÑ CHI TI·∫æT QU√Å TR√åNH X·ª¨ L√ù")
        self.add_to_report("-" * 50)
        self.add_to_report(f"T·ªïng s·ªë kh·ªëi ƒë√£ x·ª≠ l√Ω: {processing_stats['chunks_processed']}")
        self.add_to_report(f"C·ªôt ƒë√£ x√≥a: {processing_stats['columns_deleted']}")
        self.add_to_report(f"ƒê·∫∑c tr∆∞ng th·ªùi gian th√™m: {processing_stats['time_features_added']}")
        
        # C·ªôt ƒë√£ x√≥a
        original_cols = set(original_analysis['column_names'])
        processed_cols = set(processed_analysis['column_names'])
        deleted_cols = original_cols - processed_cols
        added_cols = processed_cols - original_cols
        
        if deleted_cols:
            self.add_to_report(f"\nC·ªôt ƒë√£ x√≥a ({len(deleted_cols)}):")
            for col in sorted(deleted_cols):
                self.add_to_report(f"  - {col}")
        
        if added_cols:
            self.add_to_report(f"\nC·ªôt ƒë√£ th√™m ({len(added_cols)}):")
            for col in sorted(added_cols):
                self.add_to_report(f"  - {col}")
    
    def generate_processing_phases_detail(self):
        """Chi ti·∫øt c√°c pha x·ª≠ l√Ω"""
        self.add_to_report(f"\nüîÑ CHI TI·∫æT C√ÅC PHA TI·ªÄN X·ª¨ L√ù")
        self.add_to_report("-" * 50)
        
        phases = [
            ("Pha 1", "X√≥a c·ªôt kh√¥ng c·∫ßn thi·∫øt", "Lo·∫°i b·ªè c√°c c·ªôt ID, Description, End_Time, Country, v.v."),
            ("Pha 2", "L·ªçc d·ªØ li·ªáu theo ng√†y", "Ch·ªâ gi·ªØ l·∫°i d·ªØ li·ªáu t·ª´ 2018 tr·ªü l√™n"),
            ("Pha 3", "T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian", "Th√™m c√°c c·ªôt YEAR, MONTH, DAY, HOUR, v.v."),
            ("Pha 4", "Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu SQL", "T·ªëi ∆∞u h√≥a ki·ªÉu d·ªØ li·ªáu cho SQL Server"),
            ("Pha 5", "Chu·∫©n h√≥a t√™n c·ªôt", "Chuy·ªÉn t√™n c·ªôt th√†nh ch·ªØ hoa v√† chu·∫©n h√≥a"),
            ("Pha 6", "S·∫Øp x·∫øp th·ª© t·ª± c·ªôt", "S·∫Øp x·∫øp c·ªôt theo th·ª© t·ª± DDL SQL Server")
        ]
        
        for phase_num, phase_name, description in phases:
            self.add_to_report(f"{phase_num}: {phase_name}")
            self.add_to_report(f"   {description}")
            self.add_to_report("")
    
    def generate_full_report(self, processing_stats: Dict) -> bool:
        """T·∫°o b√°o c√°o ƒë·∫ßy ƒë·ªß"""
        # Header
        self.generate_header()
        
        # Ph√¢n t√≠ch dataset g·ªëc v√† ƒë√£ x·ª≠ l√Ω
        original_analysis = self.analyze_original_dataset()
        processed_analysis = self.analyze_processed_dataset()
        
        if not original_analysis or not processed_analysis:
            self.add_to_report("‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch ƒë∆∞·ª£c c√°c dataset")
            return self.save_report()
        
        # So s√°nh chi ti·∫øt
        self.generate_comparison_report(original_analysis, processed_analysis, processing_stats)
        
        # Chi ti·∫øt c√°c pha x·ª≠ l√Ω
        self.generate_processing_phases_detail()
        
        # K·∫øt lu·∫≠n
        self.add_to_report("üéÜ K·∫æT QU·∫¢")
        self.add_to_report("-" * 50)
        self.add_to_report("‚úÖ Ti·ªÅn x·ª≠ l√Ω ho√†n th√†nh th√†nh c√¥ng!")
        self.add_to_report(f"Dataset ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho SQL Server")
        self.add_to_report(f"S·∫µn s√†ng cho vi·ªác import v√†o c∆° s·ªü d·ªØ li·ªáu")
        
        return self.save_report()

# ==========================================
# C√°c Pha Ti·ªÅn X·ª≠ L√Ω Thu·∫ßn T√∫y
# ==========================================

def phase_delete_columns(df: pd.DataFrame, columns_to_delete: List[str]) -> pd.DataFrame:
    """Pha 1: X√≥a c·ªôt kh√¥ng c·∫ßn thi·∫øt"""
    columns_to_drop = [col for col in columns_to_delete if col in df.columns]
    return df.drop(columns=columns_to_drop) if columns_to_drop else df

def phase_filter_date(df: pd.DataFrame, time_column: str = 'Start_Time', 
                     date_cutoff: str = "2018-01-01") -> pd.DataFrame:
    """Pha 2: L·ªçc d·ªØ li·ªáu theo ng√†y"""
    if time_column not in df.columns:
        return df
    
    # Convert to datetime if needed
    if df[time_column].dtype != 'datetime64[ns]':
        df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
    
    cutoff_date = pd.to_datetime(date_cutoff)
    return df[df[time_column] >= cutoff_date]

def phase_create_time_features(df: pd.DataFrame, time_column: str = 'Start_Time') -> pd.DataFrame:
    """Pha 3: T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian"""
    if time_column not in df.columns:
        return df
    
    # Chuy·ªÉn ƒë·ªïi sang datetime n·∫øu c·∫ßn
    df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
    
    # T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian c∆° b·∫£n
    df['YEAR'] = df[time_column].dt.year.astype('int16')
    df['QUARTER'] = df[time_column].dt.quarter.astype('int8')
    df['MONTH'] = df[time_column].dt.month.astype('int8')
    df['DAY'] = df[time_column].dt.day.astype('int8')
    df['HOUR'] = df[time_column].dt.hour.astype('int8')
    df['MINUTE'] = df[time_column].dt.minute.astype('int8')
    df['SECOND'] = df[time_column].dt.second.astype('int8')
    df['IS_WEEKEND'] = df[time_column].dt.dayofweek.isin([5, 6]).astype('bool')
    
    # Lo·∫°i b·ªè c·ªôt th·ªùi gian g·ªëc ƒë·ªÉ tr√°nh d∆∞ th·ª´a
    return df.drop(columns=[time_column])

def phase_sql_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 4: Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu SQL Server"""
    
    # T·ªça ƒë·ªô: decimal(9,6)
    coord_cols = ['Start_Lat', 'Start_Lng', 'LATITUDE', 'LONGITUDE']
    for col in coord_cols:
        if col in df.columns:
            df[col] = df[col].round(6).astype('float64')
    
    # S·ªë th·ª±c kh√°c: decimal(8,4) 
    float_cols = df.select_dtypes(include=['float64', 'float32']).columns
    for col in float_cols:
        if col not in coord_cols:
            df[col] = df[col].round(4).astype('float64')
    
    # S·ªë nguy√™n: int/smallint/tinyint/bit
    int_cols = df.select_dtypes(include=['int64', 'int32']).columns
    for col in int_cols:
        if col in ['YEAR']:
            df[col] = df[col].astype('int16')  # smallint
        elif col in ['QUARTER', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND']:
            df[col] = df[col].astype('int8')   # tinyint
        else:
            df[col] = df[col].astype('int32')  # int
    
    # Boolean -> bit (gi·ªØ nguy√™n ki·ªÉu bool ƒë·ªÉ analyze.py nh·∫≠n di·ªán ƒë√∫ng)
    bool_cols = df.select_dtypes(include=['bool']).columns
    
    # Danh s√°ch c√°c c·ªôt environment v√† IS_WEEKEND lu√¥n l√† Boolean (BIT)
    environment_boolean_cols = [
        'IS_WEEKEND', 'AMENITY', 'BUMP', 'CROSSING', 'GIVE_WAY', 'JUNCTION', 
        'NO_EXIT', 'RAILWAY', 'ROUNDABOUT', 'STATION', 'STOP', 
        'TRAFFIC_CALMING', 'TRAFFIC_SIGNAL', 'TURNING_LOOP'
    ]
    
    # ƒê·∫£m b·∫£o c√°c c·ªôt environment ƒë∆∞·ª£c convert v·ªÅ bool n·∫øu ch√∫ng l√† numeric
    for col in environment_boolean_cols:
        if col in df.columns:
            if df[col].dtype in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:
                # Convert v·ªÅ bool (0 -> False, non-zero -> True)
                df[col] = df[col].astype('bool')
    
    # Gi·ªØ nguy√™n t·∫•t c·∫£ c·ªôt bool ƒë·ªÉ analyze.py mapping ƒë√∫ng th√†nh BIT
    # SQL Server s·∫Ω t·ª± ƒë·ªông x·ª≠ l√Ω bool -> BIT khi import
    
    # Chu·ªói: nvarchar(4000) cho STREET, nvarchar(100) cho c√°c c·ªôt kh√°c
    string_cols = df.select_dtypes(include=['object']).columns
    for col in string_cols:
        df[col] = df[col].astype('string')  # S·ª≠ d·ª•ng string type ƒë·ªÉ mapping SQL
    
    return df

def phase_standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 5: Chu·∫©n h√≥a t√™n c·ªôt"""
    column_mapping = {}
    for col in df.columns:
        # Chuy·ªÉn t√™n c·ªôt th√†nh ch·ªØ hoa v√† chu·∫©n h√≥a
        new_col = col.upper()

        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† thay th·∫ø kho·∫£ng tr·∫Øng
        new_col = re.sub(r'\([^)]*\)', '', new_col)

        # Thay th·∫ø kho·∫£ng tr·∫Øng b·∫±ng d·∫•u g·∫°ch d∆∞·ªõi
        new_col = re.sub(r'\s+', '_', new_col.strip())

        # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë, ho·∫∑c d·∫•u g·∫°ch d∆∞·ªõi
        new_col = re.sub(r'_+', '_', new_col).strip('_')
        column_mapping[col] = new_col
    
    # ƒê·ªïi t√™n c·ªôt
    df = df.rename(columns=column_mapping)
    
    # ƒê·ªïi t√™n t·ªça ƒë·ªô c·ª• th·ªÉ
    coordinate_mapping = {'START_LAT': 'LATITUDE', 'START_LNG': 'LONGITUDE'}
    for old_name, new_name in coordinate_mapping.items():
        if old_name in df.columns:
            df = df.rename(columns={old_name: new_name})
    
    return df

def phase_reorder_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 6: S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt theo DDL SQL Server"""
    
    # ƒê·ªãnh nghƒ©a th·ª© t·ª± c·ªôt theo DDL - fact attributes l√™n ƒë·∫ßu
    fact_columns = ['SEVERITY', 'DISTANCE']
    
    # C√°c nh√≥m c·ªôt dimension theo th·ª© t·ª± DDL
    source_columns = ['SOURCE']
    
    time_columns = ['YEAR', 'QUARTER', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', 'IS_WEEKEND']
    
    location_columns = ['STATE', 'COUNTY', 'CITY', 'STREET', 'ZIPCODE', 'AIRPORT_CODE', 'TIMEZONE', 'LATITUDE', 'LONGITUDE']
    
    weather_columns = ['TEMPERATURE', 'WIND_CHILL', 'HUMIDITY', 'PRESSURE', 'VISIBILITY', 
                      'WIND_DIRECTION', 'WIND_SPEED', 'PRECIPITATION', 'WEATHER_CONDITION',
                      'SUNRISE_SUNSET', 'CIVIL_TWILIGHT', 'NAUTICAL_TWILIGHT', 'ASTRONOMICAL_TWILIGHT']
    
    environment_columns = ['AMENITY', 'BUMP', 'CROSSING', 'GIVE_WAY', 'JUNCTION', 'NO_EXIT',
                          'RAILWAY', 'ROUNDABOUT', 'STATION', 'STOP', 'TRAFFIC_CALMING',
                          'TRAFFIC_SIGNAL', 'TURNING_LOOP']
    
    # T·∫°o danh s√°ch c·ªôt theo th·ª© t·ª± mong mu·ªën
    desired_order = []
    
    # Th√™m fact columns tr∆∞·ªõc (nh·ªØng c·ªôt c√≥ trong DataFrame)
    for col in fact_columns:
        if col in df.columns:
            desired_order.append(col)
    
    # Th√™m c√°c dimension columns theo th·ª© t·ª± DDL
    for group in [source_columns, time_columns, location_columns, weather_columns, environment_columns]:
        for col in group:
            if col in df.columns:
                desired_order.append(col)
    
    # Th√™m c√°c c·ªôt c√≤n l·∫°i (n·∫øu c√≥)
    remaining_cols = [col for col in df.columns if col not in desired_order]
    desired_order.extend(remaining_cols)
    
    # S·∫Øp x·∫øp l·∫°i DataFrame theo th·ª© t·ª± mong mu·ªën
    return df[desired_order]

# ==========================================
# H√†m X·ª≠ L√Ω Ch√≠nh
# ==========================================

def get_file_info(file_path: str) -> Dict:
    """L·∫•y th√¥ng tin chi ti·∫øt file"""
    if not os.path.exists(file_path):
        return {"size_mb": 0, "exists": False}
    
    size_bytes = os.path.getsize(file_path)
    size_mb = size_bytes / (1024 * 1024)
    size_gb = size_mb / 1024
    
    return {
        "size_bytes": size_bytes,
        "size_mb": size_mb,
        "size_gb": size_gb,
        "exists": True,
        "formatted": f"{size_mb:.1f} MB" if size_mb < 1024 else f"{size_gb:.2f} GB"
    }

def process_chunks(input_file: str, output_file: str, chunk_size: int = 2600000,
                  columns_to_delete: List[str] = None, date_cutoff: str = "2018-01-01") -> Optional[Dict]:
    """X·ª≠ l√Ω d·ªØ li·ªáu theo kh·ªëi - T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô (Minimal printing)"""
    
    if columns_to_delete is None:
        columns_to_delete = ['ID', 'Description', 'End_Lat', 'End_Lng', 'End_Time', 'Weather_Timestamp', 'Country']
    
    if not os.path.exists(input_file):
        return None
    
    # X√≥a file ƒë·∫ßu ra n·∫øu c√≥
    if os.path.exists(output_file):
        os.remove(output_file)
    
    # Th·ªëng k√™ x·ª≠ l√Ω
    stats = {
        'chunks_processed': 0,
        'total_rows_input': 0,
        'total_rows_output': 0,
        'columns_deleted': 0,
        'time_features_added': 7,
        'phase_stats': {},
        'processing_log': [],
        'file_info': {
            'input_file': input_file,
            'output_file': output_file,
            'chunk_size': chunk_size,
            'date_cutoff': date_cutoff,
            'columns_to_delete': columns_to_delete
        }
    }
    
    first_chunk = True
    
    try:
        # ƒê·∫øm t·ªïng d√≤ng
        total_lines = sum(1 for _ in open(input_file, encoding='utf-8')) - 1
        total_chunks = (total_lines + chunk_size - 1) // chunk_size
        stats['processing_log'].append(f"üìä T·ªïng {total_lines:,} d√≤ng, {total_chunks} kh·ªëi")
        stats['total_lines'] = total_lines
        stats['total_chunks'] = total_chunks
        
        # X·ª≠ l√Ω t·ª´ng kh·ªëi
        chunk_reader = pd.read_csv(input_file, chunksize=chunk_size, low_memory=False)
        
        with tqdm(total=total_chunks, desc="X·ª≠ l√Ω kh·ªëi", unit="kh·ªëi") as pbar:
            for chunk_num, chunk in enumerate(chunk_reader, 1):
                initial_rows = len(chunk)
                initial_cols = len(chunk.columns)
                
                # √Åp d·ª•ng c√°c pha x·ª≠ l√Ω
                try:
                    chunk = phase_delete_columns(chunk, columns_to_delete)
                    if chunk_num == 1:
                        stats['columns_deleted'] = initial_cols - len(chunk.columns)
                    
                    chunk = phase_filter_date(chunk, date_cutoff=date_cutoff)
                    if len(chunk) == 0:
                        pbar.update(1)
                        continue
                    
                    chunk = phase_create_time_features(chunk)
                    chunk = phase_sql_data_types(chunk)
                    chunk = phase_standardize_columns(chunk)
                    chunk = phase_reorder_columns(chunk)
                    
                except Exception as e:
                    stats['processing_log'].append(f"‚ùå L·ªói x·ª≠ l√Ω kh·ªëi {chunk_num}: {e}")
                    return None
                
                # L∆∞u kh·ªëi
                try:
                    if first_chunk:
                        chunk.to_csv(output_file, index=False, mode='w')
                        first_chunk = False
                        stats['processing_log'].append(f"üìù T·∫°o file ƒë·∫ßu ra v·ªõi {len(chunk.columns)} c·ªôt")
                    else:
                        chunk.to_csv(output_file, index=False, mode='a', header=False)
                except Exception as e:
                    stats['processing_log'].append(f"‚ùå L·ªói l∆∞u kh·ªëi {chunk_num}: {e}")
                    return None
                
                # C·∫≠p nh·∫≠t th·ªëng k√™
                stats['chunks_processed'] = chunk_num
                stats['total_rows_input'] += initial_rows
                stats['total_rows_output'] += len(chunk)
                
                # C·∫≠p nh·∫≠t progress bar v·ªõi minimal info
                pbar.set_postfix({
                    'Processed': f"{stats['total_rows_output']:,}"
                })
                pbar.update(1)
                
                # D·ªçn d·∫πp b·ªô nh·ªõ
                del chunk
                gc.collect()
        
        stats['processing_log'].append("‚úÖ Ho√†n th√†nh x·ª≠ l√Ω t·∫•t c·∫£ kh·ªëi")
        return stats
        
    except Exception as e:
        if 'processing_log' in stats:
            stats['processing_log'].append(f"‚ùå L·ªói x·ª≠ l√Ω: {e}")
        return None

def analyze_dataset_detailed(file_path: str, sample_size: int = 50000) -> Optional[Dict]:
    """Ph√¢n t√≠ch chi ti·∫øt b·ªô d·ªØ li·ªáu v√† tr·∫£ v·ªÅ th√¥ng tin (No printing)"""
    if not os.path.exists(file_path):
        return None
    
    try:
        df_sample = pd.read_csv(file_path, nrows=sample_size, low_memory=False)
        total_rows = sum(1 for _ in open(file_path, encoding='utf-8')) - 1
        file_info = get_file_info(file_path)
        
        # Ph√¢n t√≠ch ki·ªÉu d·ªØ li·ªáu
        dtype_counts = df_sample.dtypes.value_counts()
        
        # Gi√° tr·ªã thi·∫øu
        missing = df_sample.isnull().sum()
        missing_info = []
        if missing.sum() > 0:
            missing_pct = (missing / len(df_sample) * 100).round(2)
            for col in missing[missing > 0].head(5).index:
                missing_info.append(f"{col}: {missing[col]:,} ({missing_pct[col]}%)")
        
        return {
            'sample_df': df_sample,
            'total_rows': total_rows,
            'file_info': file_info,
            'dtype_counts': dtype_counts.to_dict(),
            'missing_info': missing_info,
            'analysis_summary': [
                f"üìÅ File: {os.path.basename(file_path)}",
                f"üíæ K√≠ch th∆∞·ªõc: {file_info['formatted']}",
                f"üìè T·ªïng d√≤ng: {total_rows:,}",
                f"üìê T·ªïng c·ªôt: {len(df_sample.columns)}"
            ]
        }
        
    except Exception as e:
        return None

def compare_datasets_detailed(original_file: str, processed_file: str, processing_stats: Dict) -> List[str]:
    """So s√°nh chi ti·∫øt hai b·ªô d·ªØ li·ªáu v√† tr·∫£ v·ªÅ th√¥ng tin (No printing)"""
    comparison_log = []
    
    # Th√¥ng tin file
    orig_info = get_file_info(original_file)
    proc_info = get_file_info(processed_file)
    
    comparison_log.append("üîÑ SO S√ÅNH CHI TI·∫æT B·ªò D·ªÆ LI·ªÜU")
    comparison_log.append("="*70)
    comparison_log.append(f"üìÅ FILE:")
    comparison_log.append(f"  G·ªëc: {orig_info['formatted']}")
    comparison_log.append(f"  X·ª≠ l√Ω: {proc_info['formatted']}")
    
    if orig_info['exists'] and proc_info['exists']:
        reduction = ((orig_info['size_mb'] - proc_info['size_mb']) / orig_info['size_mb']) * 100
        comparison_log.append(f"  Gi·∫£m dung l∆∞·ª£ng: {reduction:.1f}%")
    
    # Th·ªëng k√™ d√≤ng
    comparison_log.append(f"üìè D·ªÆ LI·ªÜU:")
    comparison_log.append(f"  D√≤ng ƒë·∫ßu v√†o: {processing_stats['total_rows_input']:,}")
    comparison_log.append(f"  D√≤ng ƒë·∫ßu ra: {processing_stats['total_rows_output']:,}")
    row_reduction = ((processing_stats['total_rows_input'] - processing_stats['total_rows_output']) / processing_stats['total_rows_input']) * 100
    comparison_log.append(f"  Gi·∫£m d√≤ng: {row_reduction:.1f}%")
    comparison_log.append(f"  C·ªôt ƒë√£ x√≥a: {processing_stats['columns_deleted']}")
    comparison_log.append(f"  ƒê·∫∑c tr∆∞ng th·ªùi gian th√™m: {processing_stats['time_features_added']}")
    comparison_log.append(f"  Kh·ªëi ƒë√£ x·ª≠ l√Ω: {processing_stats['chunks_processed']}")
    
    return comparison_log

def parse_arguments():
    """Ph√¢n t√≠ch tham s·ªë d√≤ng l·ªánh"""
    parser = argparse.ArgumentParser(
        description='H·ªá th·ªëng ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu - T·ªëi ∆∞u h√≥a cho SQL Server (Minimal Output)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""V√≠ d·ª• s·ª≠ d·ª•ng:
  python preprocess.py ../US_Accidents_March23.csv
  python preprocess.py input.csv -o output.csv -c 2600000
  python preprocess.py data.csv --date-cutoff 2020-01-01 --chunk-size 500000
  python preprocess.py data.csv --delete-columns ID,Country,Description
        """
    )
    
    # Tham s·ªë b·∫Øt bu·ªôc
    parser.add_argument(
        'input_file', 
        help='File CSV ƒë·∫ßu v√†o (b·∫Øt bu·ªôc)'
    )
    
    # Tham s·ªë t√πy ch·ªçn
    parser.add_argument(
        '-o', '--output', 
        dest='output_file',
        help='File CSV ƒë·∫ßu ra (m·∫∑c ƒë·ªãnh: t·ª± ƒë·ªông t·∫°o t·ª´ t√™n file ƒë·∫ßu v√†o)'
    )
    
    parser.add_argument(
        '-c', '--chunk-size',
        type=int,
        default=2600000,
        help='K√≠ch th∆∞·ªõc kh·ªëi x·ª≠ l√Ω (m·∫∑c ƒë·ªãnh: 1,000,000 d√≤ng)'
    )
    
    parser.add_argument(
        '-d', '--date-cutoff',
        default="2018-01-01",
        help='Ng√†y c·∫Øt l·ªçc d·ªØ li·ªáu (m·∫∑c ƒë·ªãnh: 2018-01-01)'
    )
    
    parser.add_argument(
        '--delete-columns',
        help='Danh s√°ch c·ªôt c·∫ßn x√≥a, c√°ch nhau b·∫±ng d·∫•u ph·∫©y'
    )
    
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt'
    )
    
    parser.add_argument(
        '--version',
        action='version',
        version='Preprocess System v2.1 - Minimal Output'
    )
    
    return parser.parse_args()

def validate_arguments(args):
    """X√°c th·ª±c tham s·ªë ƒë·∫ßu v√†o (Minimal output)"""
    errors = []
    
    # Ki·ªÉm tra file ƒë·∫ßu v√†o
    if not os.path.exists(args.input_file):
        errors.append(f"‚ùå File ƒë·∫ßu v√†o kh√¥ng t·ªìn t·∫°i: {args.input_file}")
    elif not args.input_file.lower().endswith('.csv'):
        errors.append(f"‚ö†Ô∏è File ƒë·∫ßu v√†o kh√¥ng ph·∫£i CSV: {args.input_file}")
    
    # Ki·ªÉm tra chunk size
    if args.chunk_size <= 0:
        errors.append(f"‚ùå K√≠ch th∆∞·ªõc kh·ªëi ph·∫£i > 0: {args.chunk_size}")
    elif args.chunk_size < 1000:
        errors.append(f"‚ö†Ô∏è K√≠ch th∆∞·ªõc kh·ªëi qu√° nh·ªè (< 1000): {args.chunk_size}")
    
    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng ng√†y
    try:
        pd.to_datetime(args.date_cutoff)
    except:
        errors.append(f"‚ùå ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá: {args.date_cutoff}")
    
    # Ki·ªÉm tra th∆∞ m·ª•c ƒë·∫ßu ra (Silent create)
    output_dir = os.path.dirname(os.path.abspath(args.output_file))
    if not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir, exist_ok=True)
        except:
            errors.append(f"‚ùå Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c ƒë·∫ßu ra: {output_dir}")
    
    return errors

def main(input_file: str = "../US_Accidents_March23.csv",
         output_file: str = "../US_Accidents_March23-preprocessed.csv",
         chunk_size: int = 2600000,
         date_cutoff: str = "2018-01-01",
         columns_to_delete: List[str] = None) -> bool:
    """H√†m ch√≠nh (Minimal output)"""
    
    if columns_to_delete is None:
        columns_to_delete = ['ID', 'Description', 'End_Lat', 'End_Lng', 'End_Time', 'Weather_Timestamp', 'Country']
    
    # T·∫°o reporter
    reporter = PreprocessingReporter(input_file, output_file)
    
    try:
        # Ph√¢n t√≠ch d·ªØ li·ªáu g·ªëc (Silent)
        original_analysis = analyze_dataset_detailed(input_file)
        if not original_analysis:
            return False
        
        # X·ª≠ l√Ω d·ªØ li·ªáu
        processing_stats = process_chunks(
            input_file=input_file,
            output_file=output_file,
            chunk_size=chunk_size,
            columns_to_delete=columns_to_delete,
            date_cutoff=date_cutoff
        )
        
        if processing_stats is None:
            return False
        
        # Ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω (Silent)
        processed_analysis = analyze_dataset_detailed(output_file)
        
        # So s√°nh chi ti·∫øt (Silent)
        comparison_log = compare_datasets_detailed(input_file, output_file, processing_stats)
        
        # Th√™m th√¥ng tin v√†o processing stats ƒë·ªÉ report s·ª≠ d·ª•ng
        processing_stats['original_analysis'] = original_analysis
        processing_stats['processed_analysis'] = processed_analysis
        processing_stats['comparison_log'] = comparison_log
        
        # T·∫°o b√°o c√°o ti·ªÅn x·ª≠ l√Ω (Silent)
        reporter.generate_full_report(processing_stats)
        
        # Ch·ªâ print minimal th√¥ng tin cu·ªëi
        print(f"‚úÖ TI·ªÄN X·ª¨ L√ù HO√ÄN TH√ÄNH!")
        print(f"üìÅ K·∫øt qu·∫£: {output_file}")
        print(f"üìÑ B√°o c√°o: {reporter.report_file}")
        
        return True
        
    except Exception as e:
        return False

if __name__ == "__main__":
    try:
        # Ph√¢n t√≠ch tham s·ªë d√≤ng l·ªánh
        args = parse_arguments()
        
        # T·∫°o t√™n file ƒë·∫ßu ra n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
        if args.output_file is None:
            base_name = os.path.splitext(args.input_file)[0]
            args.output_file = f"{base_name}-preprocessed.csv"
        
        # X√°c th·ª±c tham s·ªë (Minimal output)
        validation_errors = validate_arguments(args)
        if validation_errors:
            for error in validation_errors:
                print(f"   {error}")
            exit(1)
        
        # X·ª≠ l√Ω danh s√°ch c·ªôt x√≥a
        if args.delete_columns:
            columns_to_delete = [col.strip() for col in args.delete_columns.split(',')]
        else:
            columns_to_delete = None
        
        # G·ªçi h√†m main v·ªõi c√°c tham s·ªë t·ª´ d√≤ng l·ªánh
        success = main(
            input_file=args.input_file,
            output_file=args.output_file,
            chunk_size=args.chunk_size,
            date_cutoff=args.date_cutoff,
            columns_to_delete=columns_to_delete
        )
        
        if not success:
            exit(1)
            
    except KeyboardInterrupt:
        exit(1)
    except Exception as e:
        exit(1)