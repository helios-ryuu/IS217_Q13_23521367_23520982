"""
H·ªá th·ªëng ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cu·ªëi c√πng - T·ªëi ∆∞u h√≥a cho SQL Server
T√°c gi·∫£: Final preprocessing system
Ng√†y: 2024
"""

import pandas as pd
import numpy as np
import warnings
import os
import gc
import re
from datetime import datetime
from typing import List, Optional, Dict
from tqdm import tqdm

# T·∫Øt c·∫£nh b√°o
warnings.filterwarnings('ignore', category=FutureWarning)
pd.options.mode.chained_assignment = None

# ==========================================
# Preprocessing Reporter
# ==========================================

class PreprocessingReporter:
    """T·∫°o b√°o c√°o qu√° tr√¨nh ti·ªÅn x·ª≠ l√Ω"""
    
    def __init__(self, input_file: str, output_file: str):
        self.input_file = input_file
        self.output_file = output_file
        # T·∫°o t√™n file report
        dataset_name = os.path.splitext(os.path.basename(input_file))[0]
        self.report_file = os.path.join(os.path.dirname(output_file), f"{dataset_name}-preprocess_report.txt")
        self.report_content = []
        
    def add_to_report(self, text: str):
        """Th√™m n·ªôi dung v√†o b√°o c√°o"""
        self.report_content.append(text)
        
    def save_report(self) -> bool:
        """L∆∞u b√°o c√°o ra file"""
        try:
            with open(self.report_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(self.report_content))
            print(f"\nüìÑ B√°o c√°o ti·ªÅn x·ª≠ l√Ω ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {self.report_file}")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u b√°o c√°o: {str(e)}")
            return False
    
    def generate_header(self):
        """T·∫°o header cho b√°o c√°o"""
        self.add_to_report("üìÑ B√ÅO C√ÅO QU√Å TR√åNH TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU")
        self.add_to_report("="*80)
        self.add_to_report(f"Ng√†y t·∫°o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.add_to_report(f"File ƒë·∫ßu v√†o: {self.input_file}")
        self.add_to_report(f"File ƒë·∫ßu ra: {self.output_file}")
        self.add_to_report("")
    
    def analyze_original_dataset(self) -> Optional[Dict]:
        """Ph√¢n t√≠ch dataset g·ªëc"""
        if not os.path.exists(self.input_file):
            return None
        
        try:
            # ƒê·ªçc m·∫´u d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch
            sample_df = pd.read_csv(self.input_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.input_file, encoding='utf-8')) - 1
            
            file_info = get_file_info(self.input_file)
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': file_info,
                'column_names': list(sample_df.columns),
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB
            }
            
            return analysis
            
        except Exception as e:
            self.add_to_report(f"‚ùå L·ªói ph√¢n t√≠ch dataset g·ªëc: {e}")
            return None
    
    def analyze_processed_dataset(self) -> Optional[Dict]:
        """Ph√¢n t√≠ch dataset ƒë√£ x·ª≠ l√Ω"""
        if not os.path.exists(self.output_file):
            return None
        
        try:
            # ƒê·ªçc m·∫´u d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch
            sample_df = pd.read_csv(self.output_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.output_file, encoding='utf-8')) - 1
            
            file_info = get_file_info(self.output_file)
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': file_info,
                'column_names': list(sample_df.columns),
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB
            }
            
            return analysis
            
        except Exception as e:
            self.add_to_report(f"‚ùå L·ªói ph√¢n t√≠ch dataset ƒë√£ x·ª≠ l√Ω: {e}")
            return None
    
    def generate_comparison_report(self, original_analysis: Dict, processed_analysis: Dict, processing_stats: Dict):
        """T·∫°o b√°o c√°o so s√°nh chi ti·∫øt"""
        
        self.add_to_report("üîç TH√îNG TIN T·ªîNG QUAN")
        self.add_to_report("-" * 50)
        
        # Th√¥ng tin c∆° b·∫£n
        self.add_to_report(f"Dataset g·ªëc:")
        self.add_to_report(f"  - S·ªë d√≤ng: {original_analysis['total_rows']:,}")
        self.add_to_report(f"  - S·ªë c·ªôt: {original_analysis['total_columns']}")
        self.add_to_report(f"  - K√≠ch th∆∞·ªõc file: {original_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - B·ªô nh·ªõ: {original_analysis['memory_usage']:.1f} MB")
        
        self.add_to_report(f"\nDataset ƒë√£ x·ª≠ l√Ω:")
        self.add_to_report(f"  - S·ªë d√≤ng: {processed_analysis['total_rows']:,}")
        self.add_to_report(f"  - S·ªë c·ªôt: {processed_analysis['total_columns']}")
        self.add_to_report(f"  - K√≠ch th∆∞·ªõc file: {processed_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - B·ªô nh·ªõ: {processed_analysis['memory_usage']:.1f} MB")
        
        # Th·ªëng k√™ thay ƒë·ªïi
        self.add_to_report(f"\nüìä THAY ƒê·ªîI SAU TI·ªÄN X·ª¨ L√ù")
        self.add_to_report("-" * 50)
        
        # Thay ƒë·ªïi s·ªë d√≤ng
        row_change = processed_analysis['total_rows'] - original_analysis['total_rows']
        row_change_pct = (row_change / original_analysis['total_rows']) * 100
        self.add_to_report(f"S·ªë d√≤ng: {row_change:+,} ({row_change_pct:+.1f}%)")
        
        # Thay ƒë·ªïi s·ªë c·ªôt
        col_change = processed_analysis['total_columns'] - original_analysis['total_columns']
        self.add_to_report(f"S·ªë c·ªôt: {col_change:+} c·ªôt")
        
        # Thay ƒë·ªïi k√≠ch th∆∞·ªõc file
        size_change_mb = processed_analysis['file_size']['size_mb'] - original_analysis['file_size']['size_mb']
        size_change_pct = (size_change_mb / original_analysis['file_size']['size_mb']) * 100
        self.add_to_report(f"K√≠ch th∆∞·ªõc file: {size_change_mb:+.1f} MB ({size_change_pct:+.1f}%)")
        
        # Thay ƒë·ªïi b·ªô nh·ªõ
        memory_change = processed_analysis['memory_usage'] - original_analysis['memory_usage']
        memory_change_pct = (memory_change / original_analysis['memory_usage']) * 100
        self.add_to_report(f"B·ªô nh·ªõ: {memory_change:+.1f} MB ({memory_change_pct:+.1f}%)")
        
        # Chi ti·∫øt c√°c pha x·ª≠ l√Ω
        self.add_to_report(f"\nüîÑ CHI TI·∫æT QU√Å TR√åNH X·ª¨ L√ù")
        self.add_to_report("-" * 50)
        self.add_to_report(f"T·ªïng s·ªë kh·ªëi ƒë√£ x·ª≠ l√Ω: {processing_stats['chunks_processed']}")
        self.add_to_report(f"C·ªôt ƒë√£ x√≥a: {processing_stats['columns_deleted']}")
        self.add_to_report(f"ƒê·∫∑c tr∆∞ng th·ªùi gian th√™m: {processing_stats['time_features_added']}")
        
        # C·ªôt ƒë√£ x√≥a
        original_cols = set(original_analysis['column_names'])
        processed_cols = set(processed_analysis['column_names'])
        deleted_cols = original_cols - processed_cols
        added_cols = processed_cols - original_cols
        
        if deleted_cols:
            self.add_to_report(f"\nC·ªôt ƒë√£ x√≥a ({len(deleted_cols)}):")
            for col in sorted(deleted_cols):
                self.add_to_report(f"  - {col}")
        
        if added_cols:
            self.add_to_report(f"\nC·ªôt ƒë√£ th√™m ({len(added_cols)}):")
            for col in sorted(added_cols):
                self.add_to_report(f"  + {col}")
    
    def generate_data_types_comparison(self, original_analysis: Dict, processed_analysis: Dict):
        """So s√°nh ki·ªÉu d·ªØ li·ªáu"""
        self.add_to_report(f"\nüè∑Ô∏è SO S√ÅNH KI·ªÇU D·ªÆ LI·ªÜU")
        self.add_to_report("-" * 50)
        
        # Ki·ªÉu d·ªØ li·ªáu g·ªëc
        self.add_to_report("Dataset g·ªëc:")
        for dtype, count in original_analysis['data_types'].items():
            self.add_to_report(f"  {str(dtype):15s}: {count:3d} c·ªôt")
        
        # Ki·ªÉu d·ªØ li·ªáu sau x·ª≠ l√Ω
        self.add_to_report("\nDataset ƒë√£ x·ª≠ l√Ω:")
        for dtype, count in processed_analysis['data_types'].items():
            self.add_to_report(f"  {str(dtype):15s}: {count:3d} c·ªôt")
    
    def generate_data_quality_comparison(self, original_analysis: Dict, processed_analysis: Dict):
        """So s√°nh ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu"""
        self.add_to_report(f"\nüîç SO S√ÅNH CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU")
        self.add_to_report("-" * 50)
        
        # Gi√° tr·ªã thi·∫øu
        self.add_to_report("Gi√° tr·ªã thi·∫øu:")
        self.add_to_report(f"  G·ªëc: {original_analysis['missing_values']:,} ({original_analysis['missing_percentage']:.2f}%)")
        self.add_to_report(f"  ƒê√£ x·ª≠ l√Ω: {processed_analysis['missing_values']:,} ({processed_analysis['missing_percentage']:.2f}%)")
        
        # B·∫£n sao
        self.add_to_report(f"\nB·∫£n sao (trong m·∫´u):")
        self.add_to_report(f"  G·ªëc: {original_analysis['duplicates']:,}")
        self.add_to_report(f"  ƒê√£ x·ª≠ l√Ω: {processed_analysis['duplicates']:,}")
    
    def generate_processing_phases_detail(self):
        """Chi ti·∫øt c√°c pha x·ª≠ l√Ω"""
        self.add_to_report(f"\nüîÑ CHI TI·∫æT C√ÅC PHA TI·ªÄN X·ª¨ L√ù")
        self.add_to_report("-" * 50)
        
        phases = [
            ("Pha 1", "X√≥a c·ªôt kh√¥ng c·∫ßn thi·∫øt", "Lo·∫°i b·ªè c√°c c·ªôt ID, Description, End_Time, v.v."),
            ("Pha 2", "L·ªçc d·ªØ li·ªáu theo ng√†y", "Ch·ªâ gi·ªØ l·∫°i d·ªØ li·ªáu t·ª´ 2018 tr·ªü l√™n"),
            ("Pha 3", "T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian", "Th√™m c√°c c·ªôt YEAR, MONTH, DAY, HOUR, v.v."),
            ("Pha 4", "Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu SQL", "T·ªëi ∆∞u h√≥a ki·ªÉu d·ªØ li·ªáu cho SQL Server"),
            ("Pha 5", "Chu·∫©n h√≥a t√™n c·ªôt", "Chuy·ªÉn t√™n c·ªôt th√†nh ch·ªØ hoa v√† chu·∫©n h√≥a"),
            ("Pha 6", "X√°c th·ª±c v√† l√†m s·∫°ch", "Lo·∫°i b·ªè b·∫£n sao v√† d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá")
        ]
        
        for phase_num, phase_name, description in phases:
            self.add_to_report(f"{phase_num}: {phase_name}")
            self.add_to_report(f"   {description}")
            self.add_to_report("")
    
    def generate_full_report(self, processing_stats: Dict) -> bool:
        """T·∫°o b√°o c√°o ƒë·∫ßy ƒë·ªß"""
        # Header
        self.generate_header()
        
        # Ph√¢n t√≠ch dataset g·ªëc v√† ƒë√£ x·ª≠ l√Ω
        original_analysis = self.analyze_original_dataset()
        processed_analysis = self.analyze_processed_dataset()
        
        if not original_analysis or not processed_analysis:
            self.add_to_report("‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch ƒë∆∞·ª£c c√°c dataset")
            return self.save_report()
        
        # So s√°nh chi ti·∫øt
        self.generate_comparison_report(original_analysis, processed_analysis, processing_stats)
        
        # So s√°nh ki·ªÉu d·ªØ li·ªáu
        self.generate_data_types_comparison(original_analysis, processed_analysis)
        
        # So s√°nh ch·∫•t l∆∞·ª£ng
        self.generate_data_quality_comparison(original_analysis, processed_analysis)
        
        # Chi ti·∫øt c√°c pha x·ª≠ l√Ω
        self.generate_processing_phases_detail()
        
        # K·∫øt lu·∫≠n
        self.add_to_report("üéÜ K·∫æT QU·∫¢")
        self.add_to_report("-" * 50)
        self.add_to_report("‚úÖ Ti·ªÅn x·ª≠ l√Ω ho√†n th√†nh th√†nh c√¥ng!")
        self.add_to_report(f"Dataset ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho SQL Server")
        self.add_to_report(f"S·∫µn s√†ng cho vi·ªác import v√†o c∆° s·ªü d·ªØ li·ªáu")
        
        return self.save_report()

# ==========================================
# Preprocessing Reporter
# ==========================================

class PreprocessingReporter:
    """T·∫°o b√°o c√°o qu√° tr√¨nh ti·ªÅn x·ª≠ l√Ω"""
    
    def __init__(self, input_file: str, output_file: str):
        self.input_file = input_file
        self.output_file = output_file
        # T·∫°o t√™n file report
        dataset_name = os.path.splitext(os.path.basename(input_file))[0]
        self.report_file = os.path.join(os.path.dirname(output_file), f"{dataset_name}-preprocess_report.txt")
        self.report_content = []
        
    def add_to_report(self, text: str):
        """Th√™m n·ªôi dung v√†o b√°o c√°o"""
        self.report_content.append(text)
        
    def save_report(self) -> bool:
        """L∆∞u b√°o c√°o ra file"""
        try:
            with open(self.report_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(self.report_content))
            print(f"\nüìÑ B√°o c√°o ti·ªÅn x·ª≠ l√Ω ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {self.report_file}")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u b√°o c√°o: {str(e)}")
            return False
    
    def generate_header(self):
        """T·∫°o header cho b√°o c√°o"""
        self.add_to_report("üìÑ B√ÅO C√ÅO QU√Å TR√åNH TI·ªÄN X·ª™L√ù D·ªÆ LI·ªÜU")
        self.add_to_report("="*80)
        self.add_to_report(f"Ng√†y t·∫°o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.add_to_report(f"File ƒë·∫ßu v√†o: {self.input_file}")
        self.add_to_report(f"File ƒë·∫ßu ra: {self.output_file}")
        self.add_to_report("")
    
    def analyze_original_dataset(self) -> Optional[Dict]:
        """Ph√¢n t√≠ch dataset g·ªëc"""
        if not os.path.exists(self.input_file):
            return None
        
        try:
            # ƒê·ªçc m·∫´u d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch
            sample_df = pd.read_csv(self.input_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.input_file, encoding='utf-8')) - 1
            
            file_info = get_file_info(self.input_file)
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': file_info,
                'column_names': list(sample_df.columns),
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB
            }
            
            return analysis
            
        except Exception as e:
            self.add_to_report(f"‚ùå L·ªói ph√¢n t√≠ch dataset g·ªëc: {e}")
            return None
    
    def analyze_processed_dataset(self) -> Optional[Dict]:
        """Ph√¢n t√≠ch dataset ƒë√£ x·ª≠ l√Ω"""
        if not os.path.exists(self.output_file):
            return None
        
        try:
            # ƒê·ªçc m·∫´u d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch
            sample_df = pd.read_csv(self.output_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.output_file, encoding='utf-8')) - 1
            
            file_info = get_file_info(self.output_file)
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': file_info,
                'column_names': list(sample_df.columns),
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB
            }
            
            return analysis
            
        except Exception as e:
            self.add_to_report(f"‚ùå L·ªói ph√¢n t√≠ch dataset ƒë√£ x·ª≠ l√Ω: {e}")
            return None
    
    def generate_comparison_report(self, original_analysis: Dict, processed_analysis: Dict, processing_stats: Dict):
        """T·∫°o b√°o c√°o so s√°nh chi ti·∫øt"""
        
        self.add_to_report("üîç TH√îNG TIN T·ªîNG QUAN")
        self.add_to_report("-" * 50)
        
        # Th√¥ng tin c∆° b·∫£n
        self.add_to_report(f"Dataset g·ªëc:")
        self.add_to_report(f"  - S·ªë d√≤ng: {original_analysis['total_rows']:,}")
        self.add_to_report(f"  - S·ªë c·ªôt: {original_analysis['total_columns']}")
        self.add_to_report(f"  - K√≠ch th∆∞·ªõc file: {original_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - B·ªô nh·ªõ: {original_analysis['memory_usage']:.1f} MB")
        
        self.add_to_report(f"\nDataset ƒë√£ x·ª≠ l√Ω:")
        self.add_to_report(f"  - S·ªë d√≤ng: {processed_analysis['total_rows']:,}")
        self.add_to_report(f"  - S·ªë c·ªôt: {processed_analysis['total_columns']}")
        self.add_to_report(f"  - K√≠ch th∆∞·ªõc file: {processed_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - B·ªô nh·ªõ: {processed_analysis['memory_usage']:.1f} MB")
        
        # Th·ªëng k√™ thay ƒë·ªïi
        self.add_to_report(f"\nüìä THAY ƒê·ªîI SAU TI·ªÄN X·ª™L√ù")
        self.add_to_report("-" * 50)
        
        # Thay ƒë·ªïi s·ªë d√≤ng
        row_change = processed_analysis['total_rows'] - original_analysis['total_rows']
        row_change_pct = (row_change / original_analysis['total_rows']) * 100
        self.add_to_report(f"S·ªë d√≤ng: {row_change:+,} ({row_change_pct:+.1f}%)")
        
        # Thay ƒë·ªïi s·ªë c·ªôt
        col_change = processed_analysis['total_columns'] - original_analysis['total_columns']
        self.add_to_report(f"S·ªë c·ªôt: {col_change:+} c·ªôt")
        
        # Thay ƒë·ªïi k√≠ch th∆∞·ªõc file
        size_change_mb = processed_analysis['file_size']['size_mb'] - original_analysis['file_size']['size_mb']
        size_change_pct = (size_change_mb / original_analysis['file_size']['size_mb']) * 100
        self.add_to_report(f"K√≠ch th∆∞·ªõc file: {size_change_mb:+.1f} MB ({size_change_pct:+.1f}%)")
        
        # Thay ƒë·ªïi b·ªô nh·ªõ
        memory_change = processed_analysis['memory_usage'] - original_analysis['memory_usage']
        memory_change_pct = (memory_change / original_analysis['memory_usage']) * 100
        self.add_to_report(f"B·ªô nh·ªõ: {memory_change:+.1f} MB ({memory_change_pct:+.1f}%)")
        
        # Chi ti·∫øt c√°c pha x·ª≠ l√Ω
        self.add_to_report(f"\nüîÑ CHI TI·∫æT QU√Å TR√åNH X·ª™ L√ù")
        self.add_to_report("-" * 50)
        self.add_to_report(f"T·ªïng s·ªë kh·ªëi ƒë√£ x·ª≠ l√Ω: {processing_stats['chunks_processed']}")
        self.add_to_report(f"C·ªôt ƒë√£ x√≥a: {processing_stats['columns_deleted']}")
        self.add_to_report(f"ƒê·∫∑c tr∆∞ng th·ªùi gian th√™m: {processing_stats['time_features_added']}")
        
        # C·ªôt ƒë√£ x√≥a
        original_cols = set(original_analysis['column_names'])
        processed_cols = set(processed_analysis['column_names'])
        deleted_cols = original_cols - processed_cols
        added_cols = processed_cols - original_cols
        
        if deleted_cols:
            self.add_to_report(f"\nC·ªôt ƒë√£ x√≥a ({len(deleted_cols)}):") 
            for col in sorted(deleted_cols):
                self.add_to_report(f"  - {col}")
        
        if added_cols:
            self.add_to_report(f"\nC·ªôt ƒë√£ th√™m ({len(added_cols)}):") 
            for col in sorted(added_cols):
                self.add_to_report(f"  + {col}")
    
    def generate_data_types_comparison(self, original_analysis: Dict, processed_analysis: Dict):
        """So s√°nh ki·ªÉu d·ªØ li·ªáu"""
        self.add_to_report(f"\nüè∑Ô∏è SO S√ÅNH KI·ªÇU D·ªÆ LI·ªÜU")
        self.add_to_report("-" * 50)
        
        # Ki·ªÉu d·ªØ li·ªáu g·ªëc
        self.add_to_report("Dataset g·ªëc:")
        for dtype, count in original_analysis['data_types'].items():
            self.add_to_report(f"  {str(dtype):15s}: {count:3d} c·ªôt")
        
        # Ki·ªÉu d·ªØ li·ªáu sau x·ª≠ l√Ω
        self.add_to_report("\nDataset ƒë√£ x·ª≠ l√Ω:")
        for dtype, count in processed_analysis['data_types'].items():
            self.add_to_report(f"  {str(dtype):15s}: {count:3d} c·ªôt")
    
    def generate_data_quality_comparison(self, original_analysis: Dict, processed_analysis: Dict):
        """So s√°nh ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu"""
        self.add_to_report(f"\nüîç SO S√ÅNH CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU")
        self.add_to_report("-" * 50)
        
        # Gi√° tr·ªã thi·∫øu
        self.add_to_report("Gi√° tr·ªã thi·∫øu:")
        self.add_to_report(f"  G·ªëc: {original_analysis['missing_values']:,} ({original_analysis['missing_percentage']:.2f}%)")
        self.add_to_report(f"  ƒê√£ x·ª≠ l√Ω: {processed_analysis['missing_values']:,} ({processed_analysis['missing_percentage']:.2f}%)")
        
        # B·∫£n sao
        self.add_to_report(f"\nB·∫£n sao (trong m·∫´u):")
        self.add_to_report(f"  G·ªëc: {original_analysis['duplicates']:,}")
        self.add_to_report(f"  ƒê√£ x·ª≠ l√Ω: {processed_analysis['duplicates']:,}")
    
    def generate_processing_phases_detail(self):
        """Chi ti·∫øt c√°c pha x·ª≠ l√Ω"""
        self.add_to_report(f"\nüîÑ CHI TI·∫æT C√ÅC PHA TI·ªÄN X·ª™L√ù")
        self.add_to_report("-" * 50)
        
        phases = [
            ("Pha 1", "X√≥a c·ªôt kh√¥ng c·∫ßn thi·∫øt", "Lo·∫°i b·ªè c√°c c·ªôt ID, Description, End_Time, v.v."),
            ("Pha 2", "L·ªçc d·ªØ li·ªáu theo ng√†y", "Ch·ªâ gi·ªØ l·∫°i d·ªØ li·ªáu t·ª´ 2018 tr·ªü l√™n"),
            ("Pha 3", "T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian", "Th√™m c√°c c·ªôt YEAR, MONTH, DAY, HOUR, v.v."),
            ("Pha 4", "Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu SQL", "T·ªëi ∆∞u h√≥a ki·ªÉu d·ªØ li·ªáu cho SQL Server"),
            ("Pha 5", "Chu·∫©n h√≥a t√™n c·ªôt", "Chuy·ªÉn t√™n c·ªôt th√†nh ch·ªØ hoa v√† chu·∫©n h√≥a"),
            ("Pha 6", "X√°c th·ª±c v√† l√†m s·∫°ch", "Lo·∫°i b·ªè b·∫£n sao v√† d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá")
        ]
        
        for phase_num, phase_name, description in phases:
            self.add_to_report(f"{phase_num}: {phase_name}")
            self.add_to_report(f"   {description}")
            self.add_to_report("")
    
    def generate_full_report(self, processing_stats: Dict) -> bool:
        """T·∫°o b√°o c√°o ƒë·∫ßy ƒë·ªß"""
        # Header
        self.generate_header()
        
        # Ph√¢n t√≠ch dataset g·ªëc v√† ƒë√£ x·ª≠ l√Ω
        original_analysis = self.analyze_original_dataset()
        processed_analysis = self.analyze_processed_dataset()
        
        if not original_analysis or not processed_analysis:
            self.add_to_report("‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch ƒë∆∞·ª£c c√°c dataset")
            return self.save_report()
        
        # So s√°nh chi ti·∫øt
        self.generate_comparison_report(original_analysis, processed_analysis, processing_stats)
        
        # So s√°nh ki·ªÉu d·ªØ li·ªáu
        self.generate_data_types_comparison(original_analysis, processed_analysis)
        
        # So s√°nh ch·∫•t l∆∞·ª£ng
        self.generate_data_quality_comparison(original_analysis, processed_analysis)
        
        # Chi ti·∫øt c√°c pha x·ª≠ l√Ω
        self.generate_processing_phases_detail()
        
        # K·∫øt lu·∫≠n
        self.add_to_report("üéÜ K·∫æT QU·∫¢")
        self.add_to_report("-" * 50)
        self.add_to_report("‚úÖ Ti·ªÅn x·ª≠ l√Ω ho√†n th√†nh th√†nh c√¥ng!")
        self.add_to_report(f"Dataset ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho SQL Server")
        self.add_to_report(f"S·∫µn s√†ng cho vi·ªác import v√†o c∆° s·ªü d·ªØ li·ªáu")
        
        return self.save_report()

# ==========================================
# C√°c Pha Ti·ªÅn X·ª≠ L√Ω Thu·∫ßn T√∫y
# ==========================================

def phase_delete_columns(df: pd.DataFrame, columns_to_delete: List[str]) -> pd.DataFrame:
    """Pha 1: X√≥a c·ªôt kh√¥ng c·∫ßn thi·∫øt"""
    columns_to_drop = [col for col in columns_to_delete if col in df.columns]
    return df.drop(columns=columns_to_drop) if columns_to_drop else df

def phase_filter_date(df: pd.DataFrame, time_column: str = 'Start_Time', 
                     date_cutoff: str = "2018-01-01") -> pd.DataFrame:
    """Pha 2: L·ªçc d·ªØ li·ªáu theo ng√†y"""
    if time_column not in df.columns:
        return df
    
    if df[time_column].dtype != 'datetime64[ns]':
        df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
    
    cutoff_date = pd.to_datetime(date_cutoff)
    return df[df[time_column] >= cutoff_date]

def phase_create_time_features(df: pd.DataFrame, time_column: str = 'Start_Time') -> pd.DataFrame:
    """Pha 3: T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian"""
    if time_column not in df.columns:
        return df
    
    # Chuy·ªÉn ƒë·ªïi sang datetime n·∫øu c·∫ßn
    df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
    
    # T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian c∆° b·∫£n
    df['YEAR'] = df[time_column].dt.year.astype('int16')
    df['QUARTER'] = df[time_column].dt.quarter.astype('int8')
    df['MONTH'] = df[time_column].dt.month.astype('int8')
    df['DAY'] = df[time_column].dt.day.astype('int8')
    df['HOUR'] = df[time_column].dt.hour.astype('int8')
    df['MINUTE'] = df[time_column].dt.minute.astype('int8')
    df['SECOND'] = df[time_column].dt.second.astype('int8')
    df['IS_WEEKEND'] = df[time_column].dt.dayofweek.isin([5, 6]).astype('bool')
    
    # Lo·∫°i b·ªè c·ªôt th·ªùi gian g·ªëc ƒë·ªÉ tr√°nh d∆∞ th·ª´a
    return df.drop(columns=[time_column])

def phase_sql_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 4: Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu SQL Server"""
    
    # T·ªça ƒë·ªô: decimal(9,6)
    coord_cols = ['Start_Lat', 'Start_Lng', 'LATITUDE', 'LONGITUDE']
    for col in coord_cols:
        if col in df.columns:
            df[col] = df[col].round(6).astype('float64')
    
    # S·ªë th·ª±c kh√°c: decimal(8,4) 
    float_cols = df.select_dtypes(include=['float64', 'float32']).columns
    for col in float_cols:
        if col not in coord_cols:
            df[col] = df[col].round(4).astype('float64')
    
    # S·ªë nguy√™n: int/smallint/tinyint/bit
    int_cols = df.select_dtypes(include=['int64', 'int32']).columns
    for col in int_cols:
        if col in ['YEAR']:
            df[col] = df[col].astype('int16')  # smallint
        elif col in ['QUARTER', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND']:
            df[col] = df[col].astype('int8')   # tinyint
        else:
            df[col] = df[col].astype('int32')  # int
    
    # Boolean -> bit
    bool_cols = df.select_dtypes(include=['bool']).columns
    for col in bool_cols:
        df[col] = df[col].astype('int8')  # bit trong SQL
    
    # Chu·ªói: nvarchar(4000) cho STREET, nvarchar(100) cho c√°c c·ªôt kh√°c
    string_cols = df.select_dtypes(include=['object']).columns
    for col in string_cols:
        df[col] = df[col].astype('string')  # S·ª≠ d·ª•ng string type ƒë·ªÉ mapping SQL
    
    return df

def phase_standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 5: Chu·∫©n h√≥a t√™n c·ªôt"""
    column_mapping = {}
    for col in df.columns:
        # Chuy·ªÉn t√™n c·ªôt th√†nh ch·ªØ hoa v√† chu·∫©n h√≥a
        new_col = col.upper()

        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† thay th·∫ø kho·∫£ng tr·∫Øng
        new_col = re.sub(r'\([^)]*\)', '', new_col)

        # Thay th·∫ø kho·∫£ng tr·∫Øng b·∫±ng d·∫•u g·∫°ch d∆∞·ªõi
        new_col = re.sub(r'\s+', '_', new_col.strip())

        # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë, ho·∫∑c d·∫•u g·∫°ch d∆∞·ªõi
        new_col = re.sub(r'_+', '_', new_col).strip('_')
        column_mapping[col] = new_col
    
    # ƒê·ªïi t√™n c·ªôt
    df = df.rename(columns=column_mapping)
    
    # ƒê·ªïi t√™n t·ªça ƒë·ªô c·ª• th·ªÉ
    coordinate_mapping = {'START_LAT': 'LATITUDE', 'START_LNG': 'LONGITUDE'}
    for old_name, new_name in coordinate_mapping.items():
        if old_name in df.columns:
            df = df.rename(columns={old_name: new_name})
    
    return df

def phase_validate_clean(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 6: X√°c th·ª±c v√† l√†m s·∫°ch d·ªØ li·ªáu"""
    
    # H√†m t√¨m c·ªôt theo m·∫´u
    def find_column(patterns: List[str]) -> Optional[str]:
        for pattern in patterns:
            for col in df.columns:
                if pattern.upper() in col.upper():
                    return col
        return None
    
    # X√°c th·ª±c m·ª©c ƒë·ªô nghi√™m tr·ªçng
    severity_col = find_column(['SEVERITY'])
    if severity_col and severity_col in df.columns:
        df = df[df[severity_col].isin([1, 2, 3, 4])]
    
    return df

# ==========================================
# H√†m X·ª≠ L√Ω Ch√≠nh
# ==========================================

def get_file_info(file_path: str) -> Dict:
    """L·∫•y th√¥ng tin chi ti·∫øt file"""
    if not os.path.exists(file_path):
        return {"size_mb": 0, "exists": False}
    
    size_bytes = os.path.getsize(file_path)
    size_mb = size_bytes / (1024 * 1024)
    size_gb = size_mb / 1024
    
    return {
        "size_bytes": size_bytes,
        "size_mb": size_mb,
        "size_gb": size_gb,
        "exists": True,
        "formatted": f"{size_mb:.1f} MB" if size_mb < 1024 else f"{size_gb:.2f} GB"
    }

def process_chunks(input_file: str, output_file: str, chunk_size: int = 2600000,
                  columns_to_delete: List[str] = None, date_cutoff: str = "2018-01-01") -> Optional[Dict]:
    """X·ª≠ l√Ω d·ªØ li·ªáu theo kh·ªëi"""
    
    if columns_to_delete is None:
        columns_to_delete = ['ID', 'Description', 'End_Lat', 'End_Lng', 'End_Time', 'Weather_Timestamp']
    
    if not os.path.exists(input_file):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {input_file}")
        return None
    
    # X√≥a file ƒë·∫ßu ra n·∫øu c√≥
    if os.path.exists(output_file):
        os.remove(output_file)
    
    # Th·ªëng k√™ x·ª≠ l√Ω
    stats = {
        'chunks_processed': 0,
        'total_rows_input': 0,
        'total_rows_output': 0,
        'columns_deleted': 0,
        'time_features_added': 7,
        'phase_stats': {}
    }
    
    first_chunk = True
    
    try:
        # ƒê·∫øm t·ªïng d√≤ng
        print("üîç ƒêang ƒë·∫øm t·ªïng s·ªë d√≤ng...")
        total_lines = sum(1 for _ in open(input_file, encoding='utf-8')) - 1
        total_chunks = (total_lines + chunk_size - 1) // chunk_size
        print(f"üìä T·ªïng {total_lines:,} d√≤ng, {total_chunks} kh·ªëi")
        
        # X·ª≠ l√Ω t·ª´ng kh·ªëi
        chunk_reader = pd.read_csv(input_file, chunksize=chunk_size, low_memory=False)
        
        with tqdm(total=total_chunks, desc="X·ª≠ l√Ω kh·ªëi", unit="kh·ªëi") as pbar:
            for chunk_num, chunk in enumerate(chunk_reader, 1):
                initial_rows = len(chunk)
                initial_cols = len(chunk.columns)
                
                # √Åp d·ª•ng c√°c pha x·ª≠ l√Ω
                try:
                    chunk = phase_delete_columns(chunk, columns_to_delete)
                    if chunk_num == 1:
                        stats['columns_deleted'] = initial_cols - len(chunk.columns)
                    
                    chunk = phase_filter_date(chunk, date_cutoff=date_cutoff)
                    if len(chunk) == 0:
                        pbar.update(1)
                        continue
                    
                    chunk = phase_create_time_features(chunk)
                    chunk = phase_sql_data_types(chunk)
                    chunk = phase_standardize_columns(chunk)
                    chunk = phase_validate_clean(chunk)
                    
                except Exception as e:
                    print(f"\n‚ùå L·ªói x·ª≠ l√Ω kh·ªëi {chunk_num}: {e}")
                    return None
                
                # L∆∞u kh·ªëi
                if first_chunk:
                    chunk.to_csv(output_file, index=False, mode='w')
                    first_chunk = False
                else:
                    chunk.to_csv(output_file, index=False, mode='a', header=False)
                
                # C·∫≠p nh·∫≠t th·ªëng k√™
                stats['chunks_processed'] = chunk_num
                stats['total_rows_input'] += initial_rows
                stats['total_rows_output'] += len(chunk)
                
                # C·∫≠p nh·∫≠t progress bar
                pbar.set_postfix({
                    'D√≤ng ƒë·∫ßu v√†o': f"{initial_rows:,}",
                    'D√≤ng ƒë·∫ßu ra': f"{len(chunk):,}",
                    'T·ªïng': f"{stats['total_rows_output']:,}"
                })
                pbar.update(1)
                
                # D·ªçn d·∫πp b·ªô nh·ªõ
                del chunk
                gc.collect()
        
        return stats
        
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω: {e}")
        return None

def analyze_dataset_detailed(file_path: str, sample_size: int = 50000) -> Optional[pd.DataFrame]:
    """Ph√¢n t√≠ch chi ti·∫øt b·ªô d·ªØ li·ªáu"""
    if not os.path.exists(file_path):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}")
        return None
    
    try:
        df_sample = pd.read_csv(file_path, nrows=sample_size, low_memory=False)
        total_rows = sum(1 for _ in open(file_path, encoding='utf-8')) - 1
        file_info = get_file_info(file_path)
        
        print(f"üìÅ File: {os.path.basename(file_path)}")
        print(f"üíæ K√≠ch th∆∞·ªõc: {file_info['formatted']}")
        print(f"üìè T·ªïng d√≤ng: {total_rows:,}")
        print(f"üìê T·ªïng c·ªôt: {len(df_sample.columns)}")
        
        # Ph√¢n t√≠ch ki·ªÉu d·ªØ li·ªáu
        print(f"\nüìä KI·ªÇU D·ªÆ LI·ªÜU:")
        dtype_counts = df_sample.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"  {dtype}: {count} c·ªôt")
        
        # Gi√° tr·ªã thi·∫øu
        missing = df_sample.isnull().sum()
        if missing.sum() > 0:
            print(f"\nüîç GI√Å TR·ªä THI·∫æU:")
            missing_pct = (missing / len(df_sample) * 100).round(2)
            for col in missing[missing > 0].head(5).index:
                print(f"  {col}: {missing[col]:,} ({missing_pct[col]}%)")
        else:
            print(f"\n‚úÖ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu!")
        
        return df_sample
        
    except Exception as e:
        print(f"‚ùå L·ªói ph√¢n t√≠ch: {e}")
        return None

def compare_datasets_detailed(original_file: str, processed_file: str, processing_stats: Dict) -> None:
    """So s√°nh chi ti·∫øt hai b·ªô d·ªØ li·ªáu"""
    print(f"\n" + "="*70)
    print("üîÑ SO S√ÅNH CHI TI·∫æT B·ªò D·ªÆ LI·ªÜU")
    print("="*70)
    
    # Th√¥ng tin file
    orig_info = get_file_info(original_file)
    proc_info = get_file_info(processed_file)
    
    print(f"üìÅ FILE:")
    print(f"  G·ªëc: {orig_info['formatted']}")
    print(f"  X·ª≠ l√Ω: {proc_info['formatted']}")
    
    if orig_info['exists'] and proc_info['exists']:
        reduction = ((orig_info['size_mb'] - proc_info['size_mb']) / orig_info['size_mb']) * 100
        print(f"  Gi·∫£m dung l∆∞·ª£ng: {reduction:.1f}%")
    
    # Th·ªëng k√™ d√≤ng
    print(f"\nüìè D·ªÆ LI·ªÜU:")
    print(f"  D√≤ng ƒë·∫ßu v√†o: {processing_stats['total_rows_input']:,}")
    print(f"  D√≤ng ƒë·∫ßu ra: {processing_stats['total_rows_output']:,}")
    row_reduction = ((processing_stats['total_rows_input'] - processing_stats['total_rows_output']) / processing_stats['total_rows_input']) * 100
    print(f"  Gi·∫£m d√≤ng: {row_reduction:.1f}%")
    print(f"  C·ªôt ƒë√£ x√≥a: {processing_stats['columns_deleted']}")
    print(f"  ƒê·∫∑c tr∆∞ng th·ªùi gian th√™m: {processing_stats['time_features_added']}")
    print(f"  Kh·ªëi ƒë√£ x·ª≠ l√Ω: {processing_stats['chunks_processed']}")

def main(input_file: str = "../US_Accidents_March23.csv",
         output_file: str = "../US_Accidents_March23-preprocessed.csv",
         chunk_size: int = 2600000,
         date_cutoff: str = "2018-01-01",
         columns_to_delete: List[str] = None) -> bool:
    """H√†m ch√≠nh"""
    
    if columns_to_delete is None:
        columns_to_delete = ['ID', 'Description', 'End_Lat', 'End_Lng', 'End_Time', 'Weather_Timestamp']
    
    print("üöÄ H·ªÜ TH·ªêNG TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU CU·ªêI C√ôNG")
    print(f"‚è∞ B·∫Øt ƒë·∫ßu: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # T·∫°o reporter
    reporter = PreprocessingReporter(input_file, output_file)
    
    # C·∫•u h√¨nh
    print("‚öôÔ∏è C·∫§U H√åNH:")
    print(f"  File ƒë·∫ßu v√†o: {input_file}")
    print(f"  File ƒë·∫ßu ra: {output_file}")
    print(f"  K√≠ch th∆∞·ªõc kh·ªëi: {chunk_size:,} d√≤ng")
    print(f"  Ng√†y c·∫Øt: {date_cutoff}")
    print(f"  C·ªôt x√≥a: {len(columns_to_delete)} c·ªôt")
    
    try:
        # Ph√¢n t√≠ch d·ªØ li·ªáu g·ªëc
        print(f"\nüìä PH√ÇN T√çCH D·ªÆ LI·ªÜU G·ªêC:")
        print("-" * 40)
        original_sample = analyze_dataset_detailed(input_file)
        
        # X·ª≠ l√Ω d·ªØ li·ªáu
        print(f"\nüîÑ X·ª¨ L√ù D·ªÆ LI·ªÜU:")
        processing_stats = process_chunks(
            input_file=input_file,
            output_file=output_file,
            chunk_size=chunk_size,
            columns_to_delete=columns_to_delete,
            date_cutoff=date_cutoff
        )
        
        if processing_stats is None:
            print("‚ùå X·ª≠ l√Ω th·∫•t b·∫°i")
            return False
        
        # Ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        print(f"\nüìä PH√ÇN T√çCH D·ªÆ LI·ªÜU ƒê√É X·ª¨ L√ù:")
        print("-" * 40)
        processed_sample = analyze_dataset_detailed(output_file)
        
        # So s√°nh chi ti·∫øt
        compare_datasets_detailed(input_file, output_file, processing_stats)
        
        # T·∫°o b√°o c√°o ti·ªÅn x·ª≠ l√Ω
        print(f"\nüìÑ T·∫†O B√ÅO C√ÅO TI·ªÄN X·ª¨ L√ù...")
        reporter.generate_full_report(processing_stats)
        
        print(f"\n" + "="*70)
        print("‚úÖ TI·ªÄN X·ª¨ L√ù HO√ÄN TH√ÄNH!")
        print(f"üìÅ K·∫øt qu·∫£: {output_file}")
        print(f"‚è∞ Ho√†n th√†nh: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üí° ƒê·ªÉ ph√¢n t√≠ch chi ti·∫øt v√† t·∫°o b√°o c√°o SQL type conversion:")
        print(f"   python analyze_dataset.py \"{output_file}\"")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)