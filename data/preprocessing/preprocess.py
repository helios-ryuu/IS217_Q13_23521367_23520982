"""
Há»‡ thá»‘ng tiá»n xá»­ lÃ½ dá»¯ liá»‡u cuá»‘i cÃ¹ng - Tá»‘i Æ°u hÃ³a cho SQL Server
TÃ¡c giáº£: Final preprocessing system
NgÃ y: 2024
"""

import pandas as pd
import numpy as np
import warnings
import os
import gc
import re
from datetime import datetime
from typing import List, Optional, Dict
from tqdm import tqdm

# Táº¯t cáº£nh bÃ¡o
warnings.filterwarnings('ignore', category=FutureWarning)
pd.options.mode.chained_assignment = None

# ==========================================
# Preprocessing Reporter
# ==========================================

class PreprocessingReporter:
    """Táº¡o bÃ¡o cÃ¡o quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½"""
    
    def __init__(self, input_file: str, output_file: str):
        self.input_file = input_file
        self.output_file = output_file
        # Táº¡o tÃªn file report
        dataset_name = os.path.splitext(os.path.basename(input_file))[0]
        self.report_file = os.path.join(os.path.dirname(output_file), f"{dataset_name}-preprocess_report.txt")
        self.report_content = []
        
    def add_to_report(self, text: str):
        """ThÃªm ná»™i dung vÃ o bÃ¡o cÃ¡o"""
        self.report_content.append(text)
        
    def save_report(self) -> bool:
        """LÆ°u bÃ¡o cÃ¡o ra file"""
        try:
            with open(self.report_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(self.report_content))
            print(f"\nğŸ“„ BÃ¡o cÃ¡o tiá»n xá»­ lÃ½ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {self.report_file}")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i khi lÆ°u bÃ¡o cÃ¡o: {str(e)}")
            return False
    
    def generate_header(self):
        """Táº¡o header cho bÃ¡o cÃ¡o"""
        self.add_to_report("ğŸ“„ BÃO CÃO QUÃ TRÃŒNH TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
        self.add_to_report("="*80)
        self.add_to_report(f"NgÃ y táº¡o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.add_to_report(f"File Ä‘áº§u vÃ o: {self.input_file}")
        self.add_to_report(f"File Ä‘áº§u ra: {self.output_file}")
        self.add_to_report("")
    
    def analyze_original_dataset(self) -> Optional[Dict]:
        """PhÃ¢n tÃ­ch dataset gá»‘c"""
        if not os.path.exists(self.input_file):
            return None
        
        try:
            # Äá»c máº«u dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch
            sample_df = pd.read_csv(self.input_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.input_file, encoding='utf-8')) - 1
            
            file_info = get_file_info(self.input_file)
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': file_info,
                'column_names': list(sample_df.columns),
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB
            }
            
            return analysis
            
        except Exception as e:
            self.add_to_report(f"âŒ Lá»—i phÃ¢n tÃ­ch dataset gá»‘c: {e}")
            return None
    
    def analyze_processed_dataset(self) -> Optional[Dict]:
        """PhÃ¢n tÃ­ch dataset Ä‘Ã£ xá»­ lÃ½"""
        if not os.path.exists(self.output_file):
            return None
        
        try:
            # Äá»c máº«u dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch
            sample_df = pd.read_csv(self.output_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.output_file, encoding='utf-8')) - 1
            
            file_info = get_file_info(self.output_file)
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': file_info,
                'column_names': list(sample_df.columns),
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB
            }
            
            return analysis
            
        except Exception as e:
            self.add_to_report(f"âŒ Lá»—i phÃ¢n tÃ­ch dataset Ä‘Ã£ xá»­ lÃ½: {e}")
            return None
    
    def generate_comparison_report(self, original_analysis: Dict, processed_analysis: Dict, processing_stats: Dict):
        """Táº¡o bÃ¡o cÃ¡o so sÃ¡nh chi tiáº¿t"""
        
        self.add_to_report("ğŸ” THÃ”NG TIN Tá»”NG QUAN")
        self.add_to_report("-" * 50)
        
        # ThÃ´ng tin cÆ¡ báº£n
        self.add_to_report(f"Dataset gá»‘c:")
        self.add_to_report(f"  - Sá»‘ dÃ²ng: {original_analysis['total_rows']:,}")
        self.add_to_report(f"  - Sá»‘ cá»™t: {original_analysis['total_columns']}")
        self.add_to_report(f"  - KÃ­ch thÆ°á»›c file: {original_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - Bá»™ nhá»›: {original_analysis['memory_usage']:.1f} MB")
        
        self.add_to_report(f"\nDataset Ä‘Ã£ xá»­ lÃ½:")
        self.add_to_report(f"  - Sá»‘ dÃ²ng: {processed_analysis['total_rows']:,}")
        self.add_to_report(f"  - Sá»‘ cá»™t: {processed_analysis['total_columns']}")
        self.add_to_report(f"  - KÃ­ch thÆ°á»›c file: {processed_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - Bá»™ nhá»›: {processed_analysis['memory_usage']:.1f} MB")
        
        # Thá»‘ng kÃª thay Ä‘á»•i
        self.add_to_report(f"\nğŸ“Š THAY Äá»”I SAU TIá»€N Xá»¬ LÃ")
        self.add_to_report("-" * 50)
        
        # Thay Ä‘á»•i sá»‘ dÃ²ng
        row_change = processed_analysis['total_rows'] - original_analysis['total_rows']
        row_change_pct = (row_change / original_analysis['total_rows']) * 100
        self.add_to_report(f"Sá»‘ dÃ²ng: {row_change:+,} ({row_change_pct:+.1f}%)")
        
        # Thay Ä‘á»•i sá»‘ cá»™t
        col_change = processed_analysis['total_columns'] - original_analysis['total_columns']
        self.add_to_report(f"Sá»‘ cá»™t: {col_change:+} cá»™t")
        
        # Thay Ä‘á»•i kÃ­ch thÆ°á»›c file
        size_change_mb = processed_analysis['file_size']['size_mb'] - original_analysis['file_size']['size_mb']
        size_change_pct = (size_change_mb / original_analysis['file_size']['size_mb']) * 100
        self.add_to_report(f"KÃ­ch thÆ°á»›c file: {size_change_mb:+.1f} MB ({size_change_pct:+.1f}%)")
        
        # Thay Ä‘á»•i bá»™ nhá»›
        memory_change = processed_analysis['memory_usage'] - original_analysis['memory_usage']
        memory_change_pct = (memory_change / original_analysis['memory_usage']) * 100
        self.add_to_report(f"Bá»™ nhá»›: {memory_change:+.1f} MB ({memory_change_pct:+.1f}%)")
        
        # Chi tiáº¿t cÃ¡c pha xá»­ lÃ½
        self.add_to_report(f"\nğŸ”„ CHI TIáº¾T QUÃ TRÃŒNH Xá»¬ LÃ")
        self.add_to_report("-" * 50)
        self.add_to_report(f"Tá»•ng sá»‘ khá»‘i Ä‘Ã£ xá»­ lÃ½: {processing_stats['chunks_processed']}")
        self.add_to_report(f"Cá»™t Ä‘Ã£ xÃ³a: {processing_stats['columns_deleted']}")
        self.add_to_report(f"Äáº·c trÆ°ng thá»i gian thÃªm: {processing_stats['time_features_added']}")
        
        # Cá»™t Ä‘Ã£ xÃ³a
        original_cols = set(original_analysis['column_names'])
        processed_cols = set(processed_analysis['column_names'])
        deleted_cols = original_cols - processed_cols
        added_cols = processed_cols - original_cols
        
        if deleted_cols:
            self.add_to_report(f"\nCá»™t Ä‘Ã£ xÃ³a ({len(deleted_cols)}):")
            for col in sorted(deleted_cols):
                self.add_to_report(f"  - {col}")
        
        if added_cols:
            self.add_to_report(f"\nCá»™t Ä‘Ã£ thÃªm ({len(added_cols)}):")
            for col in sorted(added_cols):
                self.add_to_report(f"  + {col}")
    
    def generate_data_types_comparison(self, original_analysis: Dict, processed_analysis: Dict):
        """So sÃ¡nh kiá»ƒu dá»¯ liá»‡u"""
        self.add_to_report(f"\nğŸ·ï¸ SO SÃNH KIá»‚U Dá»® LIá»†U")
        self.add_to_report("-" * 50)
        
        # Kiá»ƒu dá»¯ liá»‡u gá»‘c
        self.add_to_report("Dataset gá»‘c:")
        for dtype, count in original_analysis['data_types'].items():
            self.add_to_report(f"  {str(dtype):15s}: {count:3d} cá»™t")
        
        # Kiá»ƒu dá»¯ liá»‡u sau xá»­ lÃ½
        self.add_to_report("\nDataset Ä‘Ã£ xá»­ lÃ½:")
        for dtype, count in processed_analysis['data_types'].items():
            self.add_to_report(f"  {str(dtype):15s}: {count:3d} cá»™t")
    
    def generate_data_quality_comparison(self, original_analysis: Dict, processed_analysis: Dict):
        """So sÃ¡nh cháº¥t lÆ°á»£ng dá»¯ liá»‡u"""
        self.add_to_report(f"\nğŸ” SO SÃNH CHáº¤T LÆ¯á»¢NG Dá»® LIá»†U")
        self.add_to_report("-" * 50)
        
        # GiÃ¡ trá»‹ thiáº¿u
        self.add_to_report("GiÃ¡ trá»‹ thiáº¿u:")
        self.add_to_report(f"  Gá»‘c: {original_analysis['missing_values']:,} ({original_analysis['missing_percentage']:.2f}%)")
        self.add_to_report(f"  ÄÃ£ xá»­ lÃ½: {processed_analysis['missing_values']:,} ({processed_analysis['missing_percentage']:.2f}%)")
        
        # Báº£n sao
        self.add_to_report(f"\nBáº£n sao (trong máº«u):")
        self.add_to_report(f"  Gá»‘c: {original_analysis['duplicates']:,}")
        self.add_to_report(f"  ÄÃ£ xá»­ lÃ½: {processed_analysis['duplicates']:,}")
    
    def generate_processing_phases_detail(self):
        """Chi tiáº¿t cÃ¡c pha xá»­ lÃ½"""
        self.add_to_report(f"\nğŸ”„ CHI TIáº¾T CÃC PHA TIá»€N Xá»¬ LÃ")
        self.add_to_report("-" * 50)
        
        phases = [
            ("Pha 1", "XÃ³a cá»™t khÃ´ng cáº§n thiáº¿t", "Loáº¡i bá» cÃ¡c cá»™t ID, Description, End_Time, v.v."),
            ("Pha 2", "Lá»c dá»¯ liá»‡u theo ngÃ y", "Chá»‰ giá»¯ láº¡i dá»¯ liá»‡u tá»« 2018 trá»Ÿ lÃªn"),
            ("Pha 3", "Táº¡o Ä‘áº·c trÆ°ng thá»i gian", "ThÃªm cÃ¡c cá»™t YEAR, MONTH, DAY, HOUR, v.v."),
            ("Pha 4", "Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u SQL", "Tá»‘i Æ°u hÃ³a kiá»ƒu dá»¯ liá»‡u cho SQL Server"),
            ("Pha 5", "Chuáº©n hÃ³a tÃªn cá»™t", "Chuyá»ƒn tÃªn cá»™t thÃ nh chá»¯ hoa vÃ  chuáº©n hÃ³a"),
            ("Pha 6", "XÃ¡c thá»±c vÃ  lÃ m sáº¡ch", "Loáº¡i bá» báº£n sao vÃ  dá»¯ liá»‡u khÃ´ng há»£p lá»‡")
        ]
        
        for phase_num, phase_name, description in phases:
            self.add_to_report(f"{phase_num}: {phase_name}")
            self.add_to_report(f"   {description}")
            self.add_to_report("")
    
    def generate_full_report(self, processing_stats: Dict) -> bool:
        """Táº¡o bÃ¡o cÃ¡o Ä‘áº§y Ä‘á»§"""
        # Header
        self.generate_header()
        
        # PhÃ¢n tÃ­ch dataset gá»‘c vÃ  Ä‘Ã£ xá»­ lÃ½
        original_analysis = self.analyze_original_dataset()
        processed_analysis = self.analyze_processed_dataset()
        
        if not original_analysis or not processed_analysis:
            self.add_to_report("âŒ KhÃ´ng thá»ƒ phÃ¢n tÃ­ch Ä‘Æ°á»£c cÃ¡c dataset")
            return self.save_report()
        
        # So sÃ¡nh chi tiáº¿t
        self.generate_comparison_report(original_analysis, processed_analysis, processing_stats)
        
        # So sÃ¡nh kiá»ƒu dá»¯ liá»‡u
        self.generate_data_types_comparison(original_analysis, processed_analysis)
        
        # So sÃ¡nh cháº¥t lÆ°á»£ng
        self.generate_data_quality_comparison(original_analysis, processed_analysis)
        
        # Chi tiáº¿t cÃ¡c pha xá»­ lÃ½
        self.generate_processing_phases_detail()
        
        # Káº¿t luáº­n
        self.add_to_report("ğŸ† Káº¾T QUáº¢")
        self.add_to_report("-" * 50)
        self.add_to_report("âœ… Tiá»n xá»­ lÃ½ hoÃ n thÃ nh thÃ nh cÃ´ng!")
        self.add_to_report(f"Dataset Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho SQL Server")
        self.add_to_report(f"Sáºµn sÃ ng cho viá»‡c import vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u")
        
        return self.save_report()

# ==========================================
# Preprocessing Reporter
# ==========================================

class PreprocessingReporter:
    """Táº¡o bÃ¡o cÃ¡o quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½"""
    
    def __init__(self, input_file: str, output_file: str):
        self.input_file = input_file
        self.output_file = output_file
        # Táº¡o tÃªn file report
        dataset_name = os.path.splitext(os.path.basename(input_file))[0]
        self.report_file = os.path.join(os.path.dirname(output_file), f"{dataset_name}-preprocess_report.txt")
        self.report_content = []
        
    def add_to_report(self, text: str):
        """ThÃªm ná»™i dung vÃ o bÃ¡o cÃ¡o"""
        self.report_content.append(text)
        
    def save_report(self) -> bool:
        """LÆ°u bÃ¡o cÃ¡o ra file"""
        try:
            with open(self.report_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(self.report_content))
            print(f"\nğŸ“„ BÃ¡o cÃ¡o tiá»n xá»­ lÃ½ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {self.report_file}")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i khi lÆ°u bÃ¡o cÃ¡o: {str(e)}")
            return False
    
    def generate_header(self):
        """Táº¡o header cho bÃ¡o cÃ¡o"""
        self.add_to_report("ğŸ“„ BÃO CÃO QUÃ TRÃŒNH TIá»€N Xá»ªLÃ Dá»® LIá»†U")
        self.add_to_report("="*80)
        self.add_to_report(f"NgÃ y táº¡o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.add_to_report(f"File Ä‘áº§u vÃ o: {self.input_file}")
        self.add_to_report(f"File Ä‘áº§u ra: {self.output_file}")
        self.add_to_report("")
    
    def analyze_original_dataset(self) -> Optional[Dict]:
        """PhÃ¢n tÃ­ch dataset gá»‘c"""
        if not os.path.exists(self.input_file):
            return None
        
        try:
            # Äá»c máº«u dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch
            sample_df = pd.read_csv(self.input_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.input_file, encoding='utf-8')) - 1
            
            file_info = get_file_info(self.input_file)
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': file_info,
                'column_names': list(sample_df.columns),
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB
            }
            
            return analysis
            
        except Exception as e:
            self.add_to_report(f"âŒ Lá»—i phÃ¢n tÃ­ch dataset gá»‘c: {e}")
            return None
    
    def analyze_processed_dataset(self) -> Optional[Dict]:
        """PhÃ¢n tÃ­ch dataset Ä‘Ã£ xá»­ lÃ½"""
        if not os.path.exists(self.output_file):
            return None
        
        try:
            # Äá»c máº«u dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch
            sample_df = pd.read_csv(self.output_file, nrows=10000, low_memory=False)
            total_rows = sum(1 for _ in open(self.output_file, encoding='utf-8')) - 1
            
            file_info = get_file_info(self.output_file)
            
            analysis = {
                'total_rows': total_rows,
                'total_columns': len(sample_df.columns),
                'file_size': file_info,
                'column_names': list(sample_df.columns),
                'data_types': sample_df.dtypes.value_counts().to_dict(),
                'missing_values': sample_df.isnull().sum().sum(),
                'missing_percentage': (sample_df.isnull().sum().sum() / (len(sample_df) * len(sample_df.columns))) * 100,
                'duplicates': sample_df.duplicated().sum(),
                'memory_usage': sample_df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB
            }
            
            return analysis
            
        except Exception as e:
            self.add_to_report(f"âŒ Lá»—i phÃ¢n tÃ­ch dataset Ä‘Ã£ xá»­ lÃ½: {e}")
            return None
    
    def generate_comparison_report(self, original_analysis: Dict, processed_analysis: Dict, processing_stats: Dict):
        """Táº¡o bÃ¡o cÃ¡o so sÃ¡nh chi tiáº¿t"""
        
        self.add_to_report("ğŸ” THÃ”NG TIN Tá»”NG QUAN")
        self.add_to_report("-" * 50)
        
        # ThÃ´ng tin cÆ¡ báº£n
        self.add_to_report(f"Dataset gá»‘c:")
        self.add_to_report(f"  - Sá»‘ dÃ²ng: {original_analysis['total_rows']:,}")
        self.add_to_report(f"  - Sá»‘ cá»™t: {original_analysis['total_columns']}")
        self.add_to_report(f"  - KÃ­ch thÆ°á»›c file: {original_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - Bá»™ nhá»›: {original_analysis['memory_usage']:.1f} MB")
        
        self.add_to_report(f"\nDataset Ä‘Ã£ xá»­ lÃ½:")
        self.add_to_report(f"  - Sá»‘ dÃ²ng: {processed_analysis['total_rows']:,}")
        self.add_to_report(f"  - Sá»‘ cá»™t: {processed_analysis['total_columns']}")
        self.add_to_report(f"  - KÃ­ch thÆ°á»›c file: {processed_analysis['file_size']['formatted']}")
        self.add_to_report(f"  - Bá»™ nhá»›: {processed_analysis['memory_usage']:.1f} MB")
        
        # Thá»‘ng kÃª thay Ä‘á»•i
        self.add_to_report(f"\nğŸ“Š THAY Äá»”I SAU TIá»€N Xá»ªLÃ")
        self.add_to_report("-" * 50)
        
        # Thay Ä‘á»•i sá»‘ dÃ²ng
        row_change = processed_analysis['total_rows'] - original_analysis['total_rows']
        row_change_pct = (row_change / original_analysis['total_rows']) * 100
        self.add_to_report(f"Sá»‘ dÃ²ng: {row_change:+,} ({row_change_pct:+.1f}%)")
        
        # Thay Ä‘á»•i sá»‘ cá»™t
        col_change = processed_analysis['total_columns'] - original_analysis['total_columns']
        self.add_to_report(f"Sá»‘ cá»™t: {col_change:+} cá»™t")
        
        # Thay Ä‘á»•i kÃ­ch thÆ°á»›c file
        size_change_mb = processed_analysis['file_size']['size_mb'] - original_analysis['file_size']['size_mb']
        size_change_pct = (size_change_mb / original_analysis['file_size']['size_mb']) * 100
        self.add_to_report(f"KÃ­ch thÆ°á»›c file: {size_change_mb:+.1f} MB ({size_change_pct:+.1f}%)")
        
        # Thay Ä‘á»•i bá»™ nhá»›
        memory_change = processed_analysis['memory_usage'] - original_analysis['memory_usage']
        memory_change_pct = (memory_change / original_analysis['memory_usage']) * 100
        self.add_to_report(f"Bá»™ nhá»›: {memory_change:+.1f} MB ({memory_change_pct:+.1f}%)")
        
        # Chi tiáº¿t cÃ¡c pha xá»­ lÃ½
        self.add_to_report(f"\nğŸ”„ CHI TIáº¾T QUÃ TRÃŒNH Xá»ª LÃ")
        self.add_to_report("-" * 50)
        self.add_to_report(f"Tá»•ng sá»‘ khá»‘i Ä‘Ã£ xá»­ lÃ½: {processing_stats['chunks_processed']}")
        self.add_to_report(f"Cá»™t Ä‘Ã£ xÃ³a: {processing_stats['columns_deleted']}")
        self.add_to_report(f"Äáº·c trÆ°ng thá»i gian thÃªm: {processing_stats['time_features_added']}")
        
        # Cá»™t Ä‘Ã£ xÃ³a
        original_cols = set(original_analysis['column_names'])
        processed_cols = set(processed_analysis['column_names'])
        deleted_cols = original_cols - processed_cols
        added_cols = processed_cols - original_cols
        
        if deleted_cols:
            self.add_to_report(f"\nCá»™t Ä‘Ã£ xÃ³a ({len(deleted_cols)}):") 
            for col in sorted(deleted_cols):
                self.add_to_report(f"  - {col}")
        
        if added_cols:
            self.add_to_report(f"\nCá»™t Ä‘Ã£ thÃªm ({len(added_cols)}):") 
            for col in sorted(added_cols):
                self.add_to_report(f"  + {col}")
    
    def generate_data_types_comparison(self, original_analysis: Dict, processed_analysis: Dict):
        """So sÃ¡nh kiá»ƒu dá»¯ liá»‡u"""
        self.add_to_report(f"\nğŸ·ï¸ SO SÃNH KIá»‚U Dá»® LIá»†U")
        self.add_to_report("-" * 50)
        
        # Kiá»ƒu dá»¯ liá»‡u gá»‘c
        self.add_to_report("Dataset gá»‘c:")
        for dtype, count in original_analysis['data_types'].items():
            self.add_to_report(f"  {str(dtype):15s}: {count:3d} cá»™t")
        
        # Kiá»ƒu dá»¯ liá»‡u sau xá»­ lÃ½
        self.add_to_report("\nDataset Ä‘Ã£ xá»­ lÃ½:")
        for dtype, count in processed_analysis['data_types'].items():
            self.add_to_report(f"  {str(dtype):15s}: {count:3d} cá»™t")
    
    def generate_data_quality_comparison(self, original_analysis: Dict, processed_analysis: Dict):
        """So sÃ¡nh cháº¥t lÆ°á»£ng dá»¯ liá»‡u"""
        self.add_to_report(f"\nğŸ” SO SÃNH CHáº¤T LÆ¯á»¢NG Dá»® LIá»†U")
        self.add_to_report("-" * 50)
        
        # GiÃ¡ trá»‹ thiáº¿u
        self.add_to_report("GiÃ¡ trá»‹ thiáº¿u:")
        self.add_to_report(f"  Gá»‘c: {original_analysis['missing_values']:,} ({original_analysis['missing_percentage']:.2f}%)")
        self.add_to_report(f"  ÄÃ£ xá»­ lÃ½: {processed_analysis['missing_values']:,} ({processed_analysis['missing_percentage']:.2f}%)")
        
        # Báº£n sao
        self.add_to_report(f"\nBáº£n sao (trong máº«u):")
        self.add_to_report(f"  Gá»‘c: {original_analysis['duplicates']:,}")
        self.add_to_report(f"  ÄÃ£ xá»­ lÃ½: {processed_analysis['duplicates']:,}")
    
    def generate_processing_phases_detail(self):
        """Chi tiáº¿t cÃ¡c pha xá»­ lÃ½"""
        self.add_to_report(f"\nğŸ”„ CHI TIáº¾T CÃC PHA TIá»€N Xá»ªLÃ")
        self.add_to_report("-" * 50)
        
        phases = [
            ("Pha 1", "XÃ³a cá»™t khÃ´ng cáº§n thiáº¿t", "Loáº¡i bá» cÃ¡c cá»™t ID, Description, End_Time, v.v."),
            ("Pha 2", "Lá»c dá»¯ liá»‡u theo ngÃ y", "Chá»‰ giá»¯ láº¡i dá»¯ liá»‡u tá»« 2018 trá»Ÿ lÃªn"),
            ("Pha 3", "Táº¡o Ä‘áº·c trÆ°ng thá»i gian", "ThÃªm cÃ¡c cá»™t YEAR, MONTH, DAY, HOUR, v.v."),
            ("Pha 4", "Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u SQL", "Tá»‘i Æ°u hÃ³a kiá»ƒu dá»¯ liá»‡u cho SQL Server"),
            ("Pha 5", "Chuáº©n hÃ³a tÃªn cá»™t", "Chuyá»ƒn tÃªn cá»™t thÃ nh chá»¯ hoa vÃ  chuáº©n hÃ³a"),
            ("Pha 6", "XÃ¡c thá»±c vÃ  lÃ m sáº¡ch", "Loáº¡i bá» báº£n sao vÃ  dá»¯ liá»‡u khÃ´ng há»£p lá»‡")
        ]
        
        for phase_num, phase_name, description in phases:
            self.add_to_report(f"{phase_num}: {phase_name}")
            self.add_to_report(f"   {description}")
            self.add_to_report("")
    
    def generate_full_report(self, processing_stats: Dict) -> bool:
        """Táº¡o bÃ¡o cÃ¡o Ä‘áº§y Ä‘á»§"""
        # Header
        self.generate_header()
        
        # PhÃ¢n tÃ­ch dataset gá»‘c vÃ  Ä‘Ã£ xá»­ lÃ½
        original_analysis = self.analyze_original_dataset()
        processed_analysis = self.analyze_processed_dataset()
        
        if not original_analysis or not processed_analysis:
            self.add_to_report("âŒ KhÃ´ng thá»ƒ phÃ¢n tÃ­ch Ä‘Æ°á»£c cÃ¡c dataset")
            return self.save_report()
        
        # So sÃ¡nh chi tiáº¿t
        self.generate_comparison_report(original_analysis, processed_analysis, processing_stats)
        
        # So sÃ¡nh kiá»ƒu dá»¯ liá»‡u
        self.generate_data_types_comparison(original_analysis, processed_analysis)
        
        # So sÃ¡nh cháº¥t lÆ°á»£ng
        self.generate_data_quality_comparison(original_analysis, processed_analysis)
        
        # Chi tiáº¿t cÃ¡c pha xá»­ lÃ½
        self.generate_processing_phases_detail()
        
        # Káº¿t luáº­n
        self.add_to_report("ğŸ† Káº¾T QUáº¢")
        self.add_to_report("-" * 50)
        self.add_to_report("âœ… Tiá»n xá»­ lÃ½ hoÃ n thÃ nh thÃ nh cÃ´ng!")
        self.add_to_report(f"Dataset Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho SQL Server")
        self.add_to_report(f"Sáºµn sÃ ng cho viá»‡c import vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u")
        
        return self.save_report()

# ==========================================
# CÃ¡c Pha Tiá»n Xá»­ LÃ½ Thuáº§n TÃºy
# ==========================================

def phase_delete_columns(df: pd.DataFrame, columns_to_delete: List[str]) -> pd.DataFrame:
    """Pha 1: XÃ³a cá»™t khÃ´ng cáº§n thiáº¿t"""
    columns_to_drop = [col for col in columns_to_delete if col in df.columns]
    return df.drop(columns=columns_to_drop) if columns_to_drop else df

def phase_filter_date(df: pd.DataFrame, time_column: str = 'Start_Time', 
                     date_cutoff: str = "2018-01-01") -> pd.DataFrame:
    """Pha 2: Lá»c dá»¯ liá»‡u theo ngÃ y"""
    if time_column not in df.columns:
        return df
    
    if df[time_column].dtype != 'datetime64[ns]':
        df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
    
    cutoff_date = pd.to_datetime(date_cutoff)
    return df[df[time_column] >= cutoff_date]

def phase_create_time_features(df: pd.DataFrame, time_column: str = 'Start_Time') -> pd.DataFrame:
    """Pha 3: Táº¡o Ä‘áº·c trÆ°ng thá»i gian"""
    if time_column not in df.columns:
        return df
    
    # Chuyá»ƒn Ä‘á»•i sang datetime náº¿u cáº§n
    df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
    
    # Táº¡o Ä‘áº·c trÆ°ng thá»i gian cÆ¡ báº£n
    df['YEAR'] = df[time_column].dt.year.astype('int16')
    df['QUARTER'] = df[time_column].dt.quarter.astype('int8')
    df['MONTH'] = df[time_column].dt.month.astype('int8')
    df['DAY'] = df[time_column].dt.day.astype('int8')
    df['HOUR'] = df[time_column].dt.hour.astype('int8')
    df['MINUTE'] = df[time_column].dt.minute.astype('int8')
    df['SECOND'] = df[time_column].dt.second.astype('int8')
    df['IS_WEEKEND'] = df[time_column].dt.dayofweek.isin([5, 6]).astype('bool')
    
    # Loáº¡i bá» cá»™t thá»i gian gá»‘c Ä‘á»ƒ trÃ¡nh dÆ° thá»«a
    return df.drop(columns=[time_column])

def phase_sql_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 4: Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u SQL Server"""
    
    # Tá»a Ä‘á»™: decimal(9,6)
    coord_cols = ['Start_Lat', 'Start_Lng', 'LATITUDE', 'LONGITUDE']
    for col in coord_cols:
        if col in df.columns:
            df[col] = df[col].round(6).astype('float64')
    
    # Sá»‘ thá»±c khÃ¡c: decimal(8,4) 
    float_cols = df.select_dtypes(include=['float64', 'float32']).columns
    for col in float_cols:
        if col not in coord_cols:
            df[col] = df[col].round(4).astype('float64')
    
    # Sá»‘ nguyÃªn: int/smallint/tinyint/bit
    int_cols = df.select_dtypes(include=['int64', 'int32']).columns
    for col in int_cols:
        if col in ['YEAR']:
            df[col] = df[col].astype('int16')  # smallint
        elif col in ['QUARTER', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND']:
            df[col] = df[col].astype('int8')   # tinyint
        else:
            df[col] = df[col].astype('int32')  # int
    
    # Boolean -> bit
    bool_cols = df.select_dtypes(include=['bool']).columns
    for col in bool_cols:
        df[col] = df[col].astype('int8')  # bit trong SQL
    
    # Chuá»—i: nvarchar(4000) cho STREET, nvarchar(100) cho cÃ¡c cá»™t khÃ¡c
    string_cols = df.select_dtypes(include=['object']).columns
    for col in string_cols:
        df[col] = df[col].astype('string')  # Sá»­ dá»¥ng string type Ä‘á»ƒ mapping SQL
    
    return df

def phase_standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 5: Chuáº©n hÃ³a tÃªn cá»™t"""
    column_mapping = {}
    for col in df.columns:
        # Chuyá»ƒn tÃªn cá»™t thÃ nh chá»¯ hoa vÃ  chuáº©n hÃ³a
        new_col = col.upper()

        # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  thay tháº¿ khoáº£ng tráº¯ng
        new_col = re.sub(r'\([^)]*\)', '', new_col)

        # Thay tháº¿ khoáº£ng tráº¯ng báº±ng dáº¥u gáº¡ch dÆ°á»›i
        new_col = re.sub(r'\s+', '_', new_col.strip())

        # Loáº¡i bá» cÃ¡c kÃ½ tá»± khÃ´ng pháº£i chá»¯ cÃ¡i, sá»‘, hoáº·c dáº¥u gáº¡ch dÆ°á»›i
        new_col = re.sub(r'_+', '_', new_col).strip('_')
        column_mapping[col] = new_col
    
    # Äá»•i tÃªn cá»™t
    df = df.rename(columns=column_mapping)
    
    # Äá»•i tÃªn tá»a Ä‘á»™ cá»¥ thá»ƒ
    coordinate_mapping = {'START_LAT': 'LATITUDE', 'START_LNG': 'LONGITUDE'}
    for old_name, new_name in coordinate_mapping.items():
        if old_name in df.columns:
            df = df.rename(columns={old_name: new_name})
    
    return df

def phase_validate_clean(df: pd.DataFrame) -> pd.DataFrame:
    """Pha 6: XÃ¡c thá»±c vÃ  lÃ m sáº¡ch dá»¯ liá»‡u"""
    
    # HÃ m tÃ¬m cá»™t theo máº«u
    def find_column(patterns: List[str]) -> Optional[str]:
        for pattern in patterns:
            for col in df.columns:
                if pattern.upper() in col.upper():
                    return col
        return None
    
    # XÃ¡c thá»±c má»©c Ä‘á»™ nghiÃªm trá»ng
    severity_col = find_column(['SEVERITY'])
    if severity_col and severity_col in df.columns:
        df = df[df[severity_col].isin([1, 2, 3, 4])]
    
    return df

# ==========================================
# HÃ m Xá»­ LÃ½ ChÃ­nh
# ==========================================

def get_file_info(file_path: str) -> Dict:
    """Láº¥y thÃ´ng tin chi tiáº¿t file"""
    if not os.path.exists(file_path):
        return {"size_mb": 0, "exists": False}
    
    size_bytes = os.path.getsize(file_path)
    size_mb = size_bytes / (1024 * 1024)
    size_gb = size_mb / 1024
    
    return {
        "size_bytes": size_bytes,
        "size_mb": size_mb,
        "size_gb": size_gb,
        "exists": True,
        "formatted": f"{size_mb:.1f} MB" if size_mb < 1024 else f"{size_gb:.2f} GB"
    }

def process_chunks(input_file: str, output_file: str, chunk_size: int = 2600000,
                  columns_to_delete: List[str] = None, date_cutoff: str = "2018-01-01") -> Optional[Dict]:
    """Xá»­ lÃ½ dá»¯ liá»‡u theo khá»‘i"""
    
    if columns_to_delete is None:
        columns_to_delete = ['ID', 'Description', 'End_Lat', 'End_Lng', 'End_Time', 'Weather_Timestamp']
    
    if not os.path.exists(input_file):
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {input_file}")
        return None
    
    # XÃ³a file Ä‘áº§u ra náº¿u cÃ³
    if os.path.exists(output_file):
        os.remove(output_file)
    
    # Thá»‘ng kÃª xá»­ lÃ½
    stats = {
        'chunks_processed': 0,
        'total_rows_input': 0,
        'total_rows_output': 0,
        'columns_deleted': 0,
        'time_features_added': 7,
        'phase_stats': {}
    }
    
    first_chunk = True
    
    try:
        # Äáº¿m tá»•ng dÃ²ng
        print("ğŸ” Äang Ä‘áº¿m tá»•ng sá»‘ dÃ²ng...")
        total_lines = sum(1 for _ in open(input_file, encoding='utf-8')) - 1
        total_chunks = (total_lines + chunk_size - 1) // chunk_size
        print(f"ğŸ“Š Tá»•ng {total_lines:,} dÃ²ng, {total_chunks} khá»‘i")
        
        # Xá»­ lÃ½ tá»«ng khá»‘i
        chunk_reader = pd.read_csv(input_file, chunksize=chunk_size, low_memory=False)
        
        with tqdm(total=total_chunks, desc="Xá»­ lÃ½ khá»‘i", unit="khá»‘i") as pbar:
            for chunk_num, chunk in enumerate(chunk_reader, 1):
                initial_rows = len(chunk)
                initial_cols = len(chunk.columns)
                
                # Ãp dá»¥ng cÃ¡c pha xá»­ lÃ½
                try:
                    chunk = phase_delete_columns(chunk, columns_to_delete)
                    if chunk_num == 1:
                        stats['columns_deleted'] = initial_cols - len(chunk.columns)
                    
                    chunk = phase_filter_date(chunk, date_cutoff=date_cutoff)
                    if len(chunk) == 0:
                        pbar.update(1)
                        continue
                    
                    chunk = phase_create_time_features(chunk)
                    chunk = phase_sql_data_types(chunk)
                    chunk = phase_standardize_columns(chunk)
                    chunk = phase_validate_clean(chunk)
                    
                except Exception as e:
                    print(f"\nâŒ Lá»—i xá»­ lÃ½ khá»‘i {chunk_num}: {e}")
                    return None
                
                # LÆ°u khá»‘i
                if first_chunk:
                    chunk.to_csv(output_file, index=False, mode='w')
                    first_chunk = False
                else:
                    chunk.to_csv(output_file, index=False, mode='a', header=False)
                
                # Cáº­p nháº­t thá»‘ng kÃª
                stats['chunks_processed'] = chunk_num
                stats['total_rows_input'] += initial_rows
                stats['total_rows_output'] += len(chunk)
                
                # Cáº­p nháº­t progress bar
                pbar.set_postfix({
                    'DÃ²ng Ä‘áº§u vÃ o': f"{initial_rows:,}",
                    'DÃ²ng Ä‘áº§u ra': f"{len(chunk):,}",
                    'Tá»•ng': f"{stats['total_rows_output']:,}"
                })
                pbar.update(1)
                
                # Dá»n dáº¹p bá»™ nhá»›
                del chunk
                gc.collect()
        
        return stats
        
    except Exception as e:
        print(f"âŒ Lá»—i xá»­ lÃ½: {e}")
        return None

def analyze_dataset_detailed(file_path: str, sample_size: int = 50000) -> Optional[pd.DataFrame]:
    """PhÃ¢n tÃ­ch chi tiáº¿t bá»™ dá»¯ liá»‡u"""
    if not os.path.exists(file_path):
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {file_path}")
        return None
    
    try:
        df_sample = pd.read_csv(file_path, nrows=sample_size, low_memory=False)
        total_rows = sum(1 for _ in open(file_path, encoding='utf-8')) - 1
        file_info = get_file_info(file_path)
        
        print(f"ğŸ“ File: {os.path.basename(file_path)}")
        print(f"ğŸ’¾ KÃ­ch thÆ°á»›c: {file_info['formatted']}")
        print(f"ğŸ“ Tá»•ng dÃ²ng: {total_rows:,}")
        print(f"ğŸ“ Tá»•ng cá»™t: {len(df_sample.columns)}")
        
        # PhÃ¢n tÃ­ch kiá»ƒu dá»¯ liá»‡u
        print(f"\nğŸ“Š KIá»‚U Dá»® LIá»†U:")
        dtype_counts = df_sample.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"  {dtype}: {count} cá»™t")
        
        # GiÃ¡ trá»‹ thiáº¿u
        missing = df_sample.isnull().sum()
        if missing.sum() > 0:
            print(f"\nğŸ” GIÃ TRá»Š THIáº¾U:")
            missing_pct = (missing / len(df_sample) * 100).round(2)
            for col in missing[missing > 0].head(5).index:
                print(f"  {col}: {missing[col]:,} ({missing_pct[col]}%)")
        else:
            print(f"\nâœ… KhÃ´ng cÃ³ giÃ¡ trá»‹ thiáº¿u!")
        
        return df_sample
        
    except Exception as e:
        print(f"âŒ Lá»—i phÃ¢n tÃ­ch: {e}")
        return None

def compare_datasets_detailed(original_file: str, processed_file: str, processing_stats: Dict) -> None:
    """So sÃ¡nh chi tiáº¿t hai bá»™ dá»¯ liá»‡u"""
    print(f"\n" + "="*70)
    print("ğŸ”„ SO SÃNH CHI TIáº¾T Bá»˜ Dá»® LIá»†U")
    print("="*70)
    
    # ThÃ´ng tin file
    orig_info = get_file_info(original_file)
    proc_info = get_file_info(processed_file)
    
    print(f"ğŸ“ FILE:")
    print(f"  Gá»‘c: {orig_info['formatted']}")
    print(f"  Xá»­ lÃ½: {proc_info['formatted']}")
    
    if orig_info['exists'] and proc_info['exists']:
        reduction = ((orig_info['size_mb'] - proc_info['size_mb']) / orig_info['size_mb']) * 100
        print(f"  Giáº£m dung lÆ°á»£ng: {reduction:.1f}%")
    
    # Thá»‘ng kÃª dÃ²ng
    print(f"\nğŸ“ Dá»® LIá»†U:")
    print(f"  DÃ²ng Ä‘áº§u vÃ o: {processing_stats['total_rows_input']:,}")
    print(f"  DÃ²ng Ä‘áº§u ra: {processing_stats['total_rows_output']:,}")
    row_reduction = ((processing_stats['total_rows_input'] - processing_stats['total_rows_output']) / processing_stats['total_rows_input']) * 100
    print(f"  Giáº£m dÃ²ng: {row_reduction:.1f}%")
    print(f"  Cá»™t Ä‘Ã£ xÃ³a: {processing_stats['columns_deleted']}")
    print(f"  Äáº·c trÆ°ng thá»i gian thÃªm: {processing_stats['time_features_added']}")
    print(f"  Khá»‘i Ä‘Ã£ xá»­ lÃ½: {processing_stats['chunks_processed']}")

def main(input_file: str = "../US_Accidents_March23.csv",
         output_file: str = "../US_Accidents_March23-preprocessed.csv",
         chunk_size: int = 2600000,
         date_cutoff: str = "2018-01-01",
         columns_to_delete: List[str] = None) -> bool:
    """HÃ m chÃ­nh"""
    
    if columns_to_delete is None:
        columns_to_delete = ['ID', 'Description', 'End_Lat', 'End_Lng', 'End_Time', 'Weather_Timestamp']
    
    print("ğŸš€ Há»† THá»NG TIá»€N Xá»¬ LÃ Dá»® LIá»†U CUá»I CÃ™NG")
    print(f"â° Báº¯t Ä‘áº§u: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # Táº¡o reporter
    reporter = PreprocessingReporter(input_file, output_file)
    
    # Cáº¥u hÃ¬nh
    print("âš™ï¸ Cáº¤U HÃŒNH:")
    print(f"  File Ä‘áº§u vÃ o: {input_file}")
    print(f"  File Ä‘áº§u ra: {output_file}")
    print(f"  KÃ­ch thÆ°á»›c khá»‘i: {chunk_size:,} dÃ²ng")
    print(f"  NgÃ y cáº¯t: {date_cutoff}")
    print(f"  Cá»™t xÃ³a: {len(columns_to_delete)} cá»™t")
    
    try:
        # PhÃ¢n tÃ­ch dá»¯ liá»‡u gá»‘c
        print(f"\nğŸ“Š PHÃ‚N TÃCH Dá»® LIá»†U Gá»C:")
        print("-" * 40)
        original_sample = analyze_dataset_detailed(input_file)
        
        # Xá»­ lÃ½ dá»¯ liá»‡u
        print(f"\nğŸ”„ Xá»¬ LÃ Dá»® LIá»†U:")
        processing_stats = process_chunks(
            input_file=input_file,
            output_file=output_file,
            chunk_size=chunk_size,
            columns_to_delete=columns_to_delete,
            date_cutoff=date_cutoff
        )
        
        if processing_stats is None:
            print("âŒ Xá»­ lÃ½ tháº¥t báº¡i")
            return False
        
        # PhÃ¢n tÃ­ch dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
        print(f"\nğŸ“Š PHÃ‚N TÃCH Dá»® LIá»†U ÄÃƒ Xá»¬ LÃ:")
        print("-" * 40)
        processed_sample = analyze_dataset_detailed(output_file)
        
        # So sÃ¡nh chi tiáº¿t
        compare_datasets_detailed(input_file, output_file, processing_stats)
        
        # Táº¡o bÃ¡o cÃ¡o tiá»n xá»­ lÃ½
        print(f"\nğŸ“„ Táº O BÃO CÃO TIá»€N Xá»¬ LÃ...")
        reporter.generate_full_report(processing_stats)
        
        print(f"\n" + "="*70)
        print("âœ… TIá»€N Xá»¬ LÃ HOÃ€N THÃ€NH!")
        print(f"ğŸ“ Káº¿t quáº£: {output_file}")
        print(f"â° HoÃ n thÃ nh: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ’¡ Äá»ƒ phÃ¢n tÃ­ch chi tiáº¿t vÃ  táº¡o bÃ¡o cÃ¡o SQL type conversion:")
        print(f"   python analyze_dataset.py \"{output_file}\"")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)